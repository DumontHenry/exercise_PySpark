{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpl5cH/L9Sj5mS5Rnp2VQi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DumontHenry/exercise_PySpark/blob/main/PySpark_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaFYleaw-N0g",
        "outputId": "a5252e39-1b2e-4da4-c272-63948a4348c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.4.1\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.4.1) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285388 sha256=472a2cca39d5c3c8ff45c0c5db9715290f95da170d6ad1fd7f412a1cebc4bcc4\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n",
            "Collecting findspark==2.0.1\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark==3.4.1\n",
        "!pip install findspark==2.0.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"PySpark 101 Exercises\").getOrCreate()\n",
        "print(spark.version)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEVft8f4-eMN",
        "outputId": "dbccbf52-b73f-451a-d9dd-a82f183e74e2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# withColumn = By using PySpark withColumn() on a DataFrame, we can cast or change the data type of a column.\n",
        "# withColumn =  PySpark withColumn() function of DataFrame can also be used to change the value of an existing column.$\n",
        "# withColumn = To add/create a new column, specify the first argument with a name you want your new column to be and use the second argument to assign a value by applying an operation on an existing column\n",
        "# viwthColumn = In order to create a new column, pass the column name you wanted to the first argument of withColumn() transformation function.\n",
        "\n",
        "\n",
        "# withColumnRenamed() = Though you cannot rename a column using withColumn, still I wanted to cover this as renaming is one of the common operations we perform on DataFrame. To rename an existing column use withColumnRenamed() function on DataFrame.\n",
        "#  SparkSession.builder.appName('SparkByExamples.com').getOrCreate() = In PySpark, SparkSession.builder.appName() sets a name for your Spark application. This name is useful for identifying your application in Spark's web UI or logs"
      ],
      "metadata": {
        "id": "kMJ0-iEO66gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame([\n",
        "(\"Alice\", 1),\n",
        "(\"Bob\", 2),\n",
        "(\"Charlie\", 3),\n",
        "], [\"Name\", \"Value\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLuNXZcL_6eQ",
        "outputId": "302128a9-a2be-4d8f-f24e-d5cf35031d32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|   Name|Value|\n",
            "+-------+-----+\n",
            "|  Alice|    1|\n",
            "|    Bob|    2|\n",
            "|Charlie|    3|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Window\n",
        "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
        "\n",
        "# Applying orderBy() and monotonically_increasing_id()\n",
        "window_spec = Window.orderBy(monotonically_increasing_id())\n",
        "\n",
        "# Add a new column \"row_number\" using row_number() over the specified window\n",
        "result_df = df.withColumn(\"Index\", row_number().over(window_spec)-1)\n",
        "\n",
        "# Show the result\n",
        "result_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "vz7Sb3azAW15",
        "outputId": "806a0189-a5f3-4104-81e7-fb230393f2cc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-0e522a7f6bb9>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Add a new column \"row_number\" using row_number() over the specified window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mresult_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Index\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Show the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list1 = [\"a\", \"b\", \"c\", \"d\"]\n",
        "list2 = [1, 2, 3, 4]"
      ],
      "metadata": {
        "id": "ifhS1jxzD7N7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an RDD from the lists and convert it to a DataFrame\n",
        "rdd = spark.sparkContext.parallelize(list(zip(list1, list2)))\n",
        "df = rdd.toDF([\"Column1\", \"Column2\"])\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR7iUybmBqpN",
        "outputId": "a61b9986-4327-499c-8dc2-5b879d3ccd6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+\n",
            "|Column1|Column2|\n",
            "+-------+-------+\n",
            "|      a|      1|\n",
            "|      b|      2|\n",
            "|      c|      3|\n",
            "|      d|      4|\n",
            "+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_A = [1, 2, 3, 4, 5]\n",
        "list_B = [4, 5, 6, 7, 8]"
      ],
      "metadata": {
        "id": "myd4uB28Gcys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = spark.sparkContext\n",
        "\n",
        "rdd_A = sc.parallelize(list_A)\n",
        "rdd_B = sc.parallelize(list_B)\n",
        "\n",
        "result_rdd = rdd_A.subtract(rdd_B)\n",
        "# Collect result\n",
        "result_list = result_rdd.collect()\n",
        "print(result_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i_Yv5ebGd20",
        "outputId": "dcd371da-9ecf-4392-9b9e-30a0f7115114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_A = [1, 2, 3, 4, 5]\n",
        "list_B = [4, 5, 6, 7, 8]"
      ],
      "metadata": {
        "id": "HovLV-mZG2z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = spark.sparkContext\n",
        "\n",
        "rdd_A = sc.parallelize(list_A)\n",
        "rdd_B = sc.parallelize(list_B)\n",
        "\n",
        "result_rdd_A = rdd_A.subtract(rdd_B)\n",
        "result_rdd_B = rdd_B.subtract(rdd_A)\n",
        "\n",
        "result_rdd = result_rdd_A.union(result_rdd_B)\n",
        "# Collect result\n",
        "result_list = result_rdd.collect()\n",
        "print(result_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yA-F1aOROjnn",
        "outputId": "efb122bb-0b99-4d3d-ada8-8075b6b8bb74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 8, 6, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"A\", 10), (\"B\", 20), (\"C\", 30), (\"D\", 40), (\"E\", 50), (\"F\", 15), (\"G\", 28), (\"H\", 54), (\"I\", 41), (\"J\", 86)]\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIVZ4CD6PEkZ",
        "outputId": "66a4d7fb-3fbf-42f7-fd2f-4cb11b0241ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+\n",
            "|Name|Age|\n",
            "+----+---+\n",
            "|   A| 10|\n",
            "|   B| 20|\n",
            "|   C| 30|\n",
            "|   D| 40|\n",
            "|   E| 50|\n",
            "|   F| 15|\n",
            "|   G| 28|\n",
            "|   H| 54|\n",
            "|   I| 41|\n",
            "|   J| 86|\n",
            "+----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantiles = df.approxQuantile(\"Age\", [0.0, 0.25, 0.5, 0.75, 1.0], 0.01)\n",
        "print(quantiles)\n",
        "print(\"Min: \", quantiles[0])\n",
        "print(\"25th percentile: \", quantiles[1])\n",
        "print(\"Median: \", quantiles[2])\n",
        "print(\"75th percentile: \", quantiles[3])\n",
        "print(\"Max: \", quantiles[4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3S7qHzhPH02",
        "outputId": "2c2b8e7d-dc56-428a-baa8-e87e0349e5c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10.0, 20.0, 30.0, 50.0, 86.0]\n",
            "Min:  10.0\n",
            "25th percentile:  20.0\n",
            "Median:  30.0\n",
            "75th percentile:  50.0\n",
            "Max:  86.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "Row(name='John', job='Engineer'),\n",
        "Row(name='John', job='Engineer'),\n",
        "Row(name='Mary', job='Scientist'),\n",
        "Row(name='Bob', job='Engineer'),\n",
        "Row(name='Bob', job='Engineer'),\n",
        "Row(name='Bob', job='Scientist'),\n",
        "Row(name='Sam', job='Doctor'),\n",
        "]\n",
        "\n",
        "# create DataFrame\n",
        "df = spark.createDataFrame(data)\n",
        "\n",
        "# show DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "id": "JYI3iN3TPkGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.select('name', 'job').distinct().collect()\n",
        "####\n",
        "df.groupby('job').count().show()\n",
        "df.groupby('name').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxgGKhHYPmuf",
        "outputId": "f8bd458d-eaa5-41c2-90f3-7d82b81ebfff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----+\n",
            "|      job|count|\n",
            "+---------+-----+\n",
            "|Scientist|    2|\n",
            "| Engineer|    4|\n",
            "|   Doctor|    1|\n",
            "+---------+-----+\n",
            "\n",
            "+----+-----+\n",
            "|name|count|\n",
            "+----+-----+\n",
            "|Mary|    1|\n",
            "|John|    2|\n",
            "| Bob|    3|\n",
            "| Sam|    1|\n",
            "+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "Row(name='John', job='Engineer'),\n",
        "Row(name='John', job='Engineer'),\n",
        "Row(name='Mary', job='Scientist'),\n",
        "Row(name='Bob', job='Engineer'),\n",
        "Row(name='Bob', job='Engineer'),\n",
        "Row(name='Bob', job='Scientist'),\n",
        "Row(name='Sam', job='Doctor'),\n",
        "]\n",
        "\n",
        "# create DataFrame\n",
        "df = spark.createDataFrame(data)\n",
        "\n",
        "# show DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bl_r_E6nRAdB",
        "outputId": "2c45cdd0-bb19-4fe4-fb3e-ccb64ee36cb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "|name|      job|\n",
            "+----+---------+\n",
            "|John| Engineer|\n",
            "|John| Engineer|\n",
            "|Mary|Scientist|\n",
            "| Bob| Engineer|\n",
            "| Bob| Engineer|\n",
            "| Bob|Scientist|\n",
            "| Sam|   Doctor|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "# Get the top 2 most frequent jobs\n",
        "top_2_jobs= df.groupby('job').count().orderBy('count', ascending=False).limit(2).select('job').rdd.flatMap(lambda x: x).collect()\n",
        "# Replace all but the top 2 most frequent jobs with 'Other'\n",
        "df = df.withColumn('job', when(col('job').isin(top_2_jobs), col('job')).otherwise('Other'))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7l1uKN_RHzv",
        "outputId": "77fa59d4-aab8-4287-cb60-6e866a126014"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "|name|      job|\n",
            "+----+---------+\n",
            "|John| Engineer|\n",
            "|John| Engineer|\n",
            "|Mary|Scientist|\n",
            "| Bob| Engineer|\n",
            "| Bob| Engineer|\n",
            "| Bob|Scientist|\n",
            "| Sam|    Other|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df is your DataFrame\n",
        "df = spark.createDataFrame([\n",
        "(\"A\", 1, None),\n",
        "(\"B\", None, \"123\" ),\n",
        "(\"B\", 3, \"456\"),\n",
        "(\"D\", None, None),\n",
        "], [\"Name\", \"Value\", \"id\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrFWQe9DSk4o",
        "outputId": "115dfcc8-90f4-4b85-c1f2-d72da5eab61c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+----+\n",
            "|Name|Value|  id|\n",
            "+----+-----+----+\n",
            "|   A|    1|null|\n",
            "|   B| null| 123|\n",
            "|   B|    3| 456|\n",
            "|   D| null|null|\n",
            "+----+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_1 = df.dropna(subset=['Value'], how='all')\n",
        "df_1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sfahV27TEsO",
        "outputId": "78f93357-b2d9-45f6-a8d4-0aa1c3c18f5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+----+\n",
            "|Name|Value|  id|\n",
            "+----+-----+----+\n",
            "|   A|    1|null|\n",
            "|   B|    3| 456|\n",
            "+----+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# suppose you have the following DataFrame\n",
        "df = spark.createDataFrame([(1, 2, 3), (4, 5, 6)], [\"col1\", \"col2\", \"col3\"])\n",
        "\n",
        "# old column names\n",
        "old_names = [\"col1\", \"col2\", \"col3\"]\n",
        "\n",
        "# new column names\n",
        "new_names = [\"new_col1\", \"new_col2\", \"new_col3\"]\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuvJjLIIUOZ3",
        "outputId": "b9526e8f-ad83-4f85-e4d3-5a7acb2ce2d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+\n",
            "|col1|col2|col3|\n",
            "+----+----+----+\n",
            "|   1|   2|   3|\n",
            "|   4|   5|   6|\n",
            "+----+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for old_name, new_name in zip(old_names, new_names):\n",
        "  df = df.withColumnRenamed(old_name, new_name )\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvYqVS9uUgdT",
        "outputId": "32a9adfa-8bb1-47bc-e2db-9111c7a3e32a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+--------+\n",
            "|new_col1|new_col2|new_col3|\n",
            "+--------+--------+--------+\n",
            "|       1|       2|       3|\n",
            "|       4|       5|       6|\n",
            "+--------+--------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rand\n",
        "from pyspark.ml.feature import Bucketizer\n",
        "\n",
        "# Create a DataFrame with a single column \"values\" filled with random numbers\n",
        "num_items = 100\n",
        "df = spark.range(num_items).select(rand(seed=42).alias(\"values\"))\n",
        "\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bB6g0gfWUw98",
        "outputId": "3f7228a0-d455-47c5-fa14-5544b81fc300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "|             values|\n",
            "+-------------------+\n",
            "|  0.619189370225301|\n",
            "| 0.5096018842446481|\n",
            "| 0.8325259388871524|\n",
            "|0.26322809041172357|\n",
            "| 0.6702867696264135|\n",
            "+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_buckets = 10  # Number of buckets\n",
        "quantiles = df.stat.approxQuantile(\"values\",[i/num_buckets for i in range(num_buckets+1)], 0.01)\n",
        "\n",
        "# Create the Bucketizer\n",
        "bucketizer = Bucketizer(splits=quantiles, inputCol=\"values\", outputCol=\"buckets\")\n",
        "\n",
        "# Apply the Bucketizer\n",
        "df_buck = bucketizer.transform(df)\n",
        "\n",
        "#Frequency table\n",
        "df_buck.groupBy(\"buckets\").count().show()\n",
        "\n",
        "# Show the original and bucketed values\n",
        "df_buck.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-khphvLWDD8",
        "outputId": "6c1204a9-b0b4-432b-a7be-877dcdf42129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|buckets|count|\n",
            "+-------+-----+\n",
            "|    8.0|   10|\n",
            "|    0.0|    8|\n",
            "|    7.0|   10|\n",
            "|    1.0|   10|\n",
            "|    4.0|   10|\n",
            "|    3.0|   10|\n",
            "|    2.0|   10|\n",
            "|    6.0|   10|\n",
            "|    5.0|   10|\n",
            "|    9.0|   12|\n",
            "+-------+-----+\n",
            "\n",
            "+-------------------+-------+\n",
            "|             values|buckets|\n",
            "+-------------------+-------+\n",
            "|  0.619189370225301|    4.0|\n",
            "| 0.5096018842446481|    4.0|\n",
            "| 0.8325259388871524|    8.0|\n",
            "|0.26322809041172357|    2.0|\n",
            "| 0.6702867696264135|    5.0|\n",
            "+-------------------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example DataFrame\n",
        "data = [(\"A\", \"X\"), (\"A\", \"Y\"), (\"A\", \"X\"), (\"B\", \"Y\"), (\"B\", \"X\"), (\"C\", \"X\"), (\"C\", \"X\"), (\"C\", \"Y\")]\n",
        "df = spark.createDataFrame(data, [\"category1\", \"category2\"])\n",
        "\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmg3shZGWeJh",
        "outputId": "b465b152-f17c-4237-cd35-8f55494e0028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+\n",
            "|category1|category2|\n",
            "+---------+---------+\n",
            "|        A|        X|\n",
            "|        A|        Y|\n",
            "|        A|        X|\n",
            "|        B|        Y|\n",
            "|        B|        X|\n",
            "|        C|        X|\n",
            "|        C|        X|\n",
            "|        C|        Y|\n",
            "+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.cube(\"category1\").count().show()\n",
        "\"\"\"\n",
        "The cube function in PySpark is used to perform data aggregation by generating all possible combinations of grouping columns, similar to how the groupby function works.\n",
        "However, cube goes a step further by also including aggregations for all subsets of the specified grouping columns, as well as the grand total (aggregation across all data).\n",
        "This means that if you apply cube on multiple columns, it will generate aggregations for each individual column, all possible pairs, all possible triplets, and so on, along with the overall total.\n",
        "For example, in the code you provided, df.cube(\"category1\").count().show(), the cube function is applied to the \"category1\" column.\n",
        "This will generate counts for each distinct value in \"category1\" as well as a total count for all categories combined.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnDem-E6Wg23",
        "outputId": "12b18703-951e-4665-fafe-a7dbb4d67b70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----+\n",
            "|category1|count|\n",
            "+---------+-----+\n",
            "|        B|    2|\n",
            "|     null|    8|\n",
            "|        A|    3|\n",
            "|        C|    3|\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Contingency table\n",
        "df.crosstab('category1', 'category2').show()\n",
        "\"\"\"\n",
        "The crosstab function in PySpark is used to compute a contingency table (also known as a cross-tabulation) for two columns of a DataFrame.\n",
        " It essentially creates a frequency table that shows the distribution of values in one column, categorized by the values in another column.\n",
        "In your code, df.crosstab('category1', 'category2').show(), it generates a table that shows how many times each combination of values from \"category1\" and \"category2\" appears in your DataFrame.\n",
        "This allows you to see the relationship or association between those two categorical variables\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQLTaMRVXXMy",
        "outputId": "c6f8f9e5-a228-454e-ecda-19e4fe9fcbf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+---+---+\n",
            "|category1_category2|  X|  Y|\n",
            "+-------------------+---+---+\n",
            "|                  B|  1|  1|\n",
            "|                  C|  2|  1|\n",
            "|                  A|  2|  1|\n",
            "+-------------------+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rand\n",
        "\n",
        "# Generate a DataFrame with a single column \"id\" with 10 rows\n",
        "df = spark.range(10)\n",
        "# Generate a random float between 0 and 1, scale and shift it to get a random integer between 1 and 10\n",
        "df = df.withColumn(\"random\", ((rand(seed=42) * 10) + 1).cast(\"int\"))\n",
        "# Show the DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyVru8o1X2Hn",
        "outputId": "fca48dcd-911a-437e-a347-39bab9138c17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| id|random|\n",
            "+---+------+\n",
            "|  0|     7|\n",
            "|  1|     6|\n",
            "|  2|     9|\n",
            "|  3|     3|\n",
            "|  4|     7|\n",
            "|  5|     9|\n",
            "|  6|     7|\n",
            "|  7|     3|\n",
            "|  8|     3|\n",
            "|  9|     7|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "df = df.withColumn(\"is_multiple_of_3\", when(col(\"random\")%3==0, 1).otherwise(0))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4lA7MyRZBr_",
        "outputId": "fc2d7313-bc84-4e48-eb1b-52f6bbf6879c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+----------------+\n",
            "| id|random|is_multiple_of_3|\n",
            "+---+------+----------------+\n",
            "|  0|     7|               0|\n",
            "|  1|     6|               1|\n",
            "|  2|     9|               1|\n",
            "|  3|     3|               1|\n",
            "|  4|     7|               0|\n",
            "|  5|     9|               1|\n",
            "|  6|     7|               0|\n",
            "|  7|     3|               1|\n",
            "|  8|     3|               1|\n",
            "|  9|     7|               0|\n",
            "+---+------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rand\n",
        "\n",
        "# Generate a DataFrame with a single column \"id\" with 10 rows\n",
        "df = spark.range(10)\n",
        "\n",
        "# Generate a random float between 0 and 1, scale and shift it to get a random integer between 1 and 10\n",
        "df = df.withColumn(\"random\", ((rand(seed=42) * 10) + 1).cast(\"int\"))\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()\n",
        "\n",
        "pos = [0, 4, 8, 5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmOT-afiaV4H",
        "outputId": "3f1a7cf7-d8cf-4903-bf0d-13add16462d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| id|random|\n",
            "+---+------+\n",
            "|  0|     7|\n",
            "|  1|     6|\n",
            "|  2|     9|\n",
            "|  3|     3|\n",
            "|  4|     7|\n",
            "|  5|     9|\n",
            "|  6|     7|\n",
            "|  7|     3|\n",
            "|  8|     3|\n",
            "|  9|     7|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos = [0, 4, 8, 5]\n",
        "\n",
        "# Define window specification\n",
        "w = Window.orderBy(monotonically_increasing_id())\n",
        "\n",
        "# Add index\n",
        "df = df.withColumn(\"index\", row_number().over(w) - 1)\n",
        "\n",
        "df.show()\n",
        "\n",
        "# Filter the DataFrame based on the specified positions\n",
        "df_filtered = df.filter(df.index.isin(pos))\n",
        "\n",
        "df_filtered.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImR9hKuwbdnq",
        "outputId": "7f76db72-8b57-40cb-92ae-6ef570aa688a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+-----+\n",
            "| id|random|index|\n",
            "+---+------+-----+\n",
            "|  0|     7|    0|\n",
            "|  1|     6|    1|\n",
            "|  2|     9|    2|\n",
            "|  3|     3|    3|\n",
            "|  4|     7|    4|\n",
            "|  5|     9|    5|\n",
            "|  6|     7|    6|\n",
            "|  7|     3|    7|\n",
            "|  8|     3|    8|\n",
            "|  9|     7|    9|\n",
            "+---+------+-----+\n",
            "\n",
            "+---+------+-----+\n",
            "| id|random|index|\n",
            "+---+------+-----+\n",
            "|  0|     7|    0|\n",
            "|  4|     7|    4|\n",
            "|  5|     9|    5|\n",
            "|  8|     3|    8|\n",
            "+---+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_A = spark.createDataFrame([(\"apple\", 3, 5), (\"banana\", 1, 10), (\"orange\", 2, 8)], [\"Name\", \"Col_1\", \"Col_2\"])\n",
        "df_A.show()\n",
        "\n",
        "# Create DataFrame for region B\n",
        "df_B = spark.createDataFrame([(\"apple\", 3, 5), (\"banana\", 1, 15), (\"grape\", 4, 6)], [\"Name\", \"Col_1\", \"Col_3\"])\n",
        "df_B.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDdT7sQOc9fP",
        "outputId": "56adc0a3-ee6c-456c-deee-536f7d6dc3ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-----+\n",
            "|  Name|Col_1|Col_2|\n",
            "+------+-----+-----+\n",
            "| apple|    3|    5|\n",
            "|banana|    1|   10|\n",
            "|orange|    2|    8|\n",
            "+------+-----+-----+\n",
            "\n",
            "+------+-----+-----+\n",
            "|  Name|Col_1|Col_3|\n",
            "+------+-----+-----+\n",
            "| apple|    3|    5|\n",
            "|banana|    1|   15|\n",
            "| grape|    4|    6|\n",
            "+------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df= df_A.union(df_B)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovqTLk6Ohsxo",
        "outputId": "97abc535-1648-454c-e918-387d57ed83af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-----+\n",
            "|  Name|Col_1|Col_2|\n",
            "+------+-----+-----+\n",
            "| apple|    3|    5|\n",
            "|banana|    1|   10|\n",
            "|orange|    2|    8|\n",
            "| apple|    3|    5|\n",
            "|banana|    1|   15|\n",
            "| grape|    4|    6|\n",
            "+------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume you have a DataFrame df with two columns \"actual\" and \"predicted\"\n",
        "# For the sake of example, we'll create a sample DataFrame\n",
        "data = [(1, 1), (2, 4), (3, 9), (4, 16), (5, 25)]\n",
        "df = spark.createDataFrame(data, [\"actual\", \"predicted\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MptRWLJFvgKR",
        "outputId": "d2af3265-9db2-4c16-92d3-a554427168c4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+\n",
            "|actual|predicted|\n",
            "+------+---------+\n",
            "|     1|        1|\n",
            "|     2|        4|\n",
            "|     3|        9|\n",
            "|     4|       16|\n",
            "|     5|       25|\n",
            "+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when\n",
        "df= df.withColumn(\"squarred_error\", pow(col(\"actual\")-col(\"predicted\"), 2))\n",
        "mse = df.agg({\"squarred_error\": \"avg\"}).collect()[0][0]\n",
        "df.show()\n",
        "print(f\"Mean Squared Error (MSE) = {mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md_2XwnzwMCW",
        "outputId": "5272e3b6-a498-4e92-eb20-a0ba0d0836fc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+--------------+\n",
            "|actual|predicted|squarred_error|\n",
            "+------+---------+--------------+\n",
            "|     1|        1|           0.0|\n",
            "|     2|        4|           4.0|\n",
            "|     3|        9|          36.0|\n",
            "|     4|       16|         144.0|\n",
            "|     5|       25|         400.0|\n",
            "+------+---------+--------------+\n",
            "\n",
            "Mean Squared Error (MSE) = 116.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **17. How to convert the first character of each element in a series to uppercase?**"
      ],
      "metadata": {
        "id": "gcFEeuCi5zEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppose you have the following DataFrame\n",
        "data = [(\"john\",), (\"alice\",), (\"bob\",)]\n",
        "df = spark.createDataFrame(data, [\"name\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcivcwM5xSmf",
        "outputId": "d311492b-b91e-4124-8ad3-71e06d21ba24"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+\n",
            "| name|\n",
            "+-----+\n",
            "| john|\n",
            "|alice|\n",
            "|  bob|\n",
            "+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "df.withColumn(\"name\", initcap(col(\"name\"))).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbdhPcdwxU9O",
        "outputId": "287a960a-f66f-4fe5-dc25-9b62b138f952"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+\n",
            "| name|\n",
            "+-----+\n",
            "| John|\n",
            "|Alice|\n",
            "|  Bob|\n",
            "+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **18. How to compute summary statistics for all columns in a dataframe**"
      ],
      "metadata": {
        "id": "Oy5Ik8-J5tTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For the sake of example, we'll create a sample DataFrame\n",
        "data = [('James', 34, 55000),\n",
        "('Michael', 30, 70000),\n",
        "('Robert', 37, 60000),\n",
        "('Maria', 29, 80000),\n",
        "('Jen', 32, 65000)]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\" , \"salary\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eko96r0fx_J4",
        "outputId": "5d138e17-8018-4b2e-9d2e-79ebb2e9246e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+\n",
            "|   name|age|salary|\n",
            "+-------+---+------+\n",
            "|  James| 34| 55000|\n",
            "|Michael| 30| 70000|\n",
            "| Robert| 37| 60000|\n",
            "|  Maria| 29| 80000|\n",
            "|    Jen| 32| 65000|\n",
            "+-------+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary = df.summary()\n",
        "summary.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwpPQOWa5dce",
        "outputId": "6490e5d7-9bc3-4746-fe5d-1253fed580d5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+-----------------+-----------------+\n",
            "|summary|  name|              age|           salary|\n",
            "+-------+------+-----------------+-----------------+\n",
            "|  count|     5|                5|                5|\n",
            "|   mean|  null|             32.4|          66000.0|\n",
            "| stddev|  null|3.209361307176242|9617.692030835671|\n",
            "|    min| James|               29|            55000|\n",
            "|    25%|  null|               30|            60000|\n",
            "|    50%|  null|               32|            65000|\n",
            "|    75%|  null|               34|            70000|\n",
            "|    max|Robert|               37|            80000|\n",
            "+-------+------+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **19. How to calculate the number of characters in each word in a column?**"
      ],
      "metadata": {
        "id": "piaAfnkr53im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppose you have the following DataFrame\n",
        "data = [(\"john\",), (\"alice\",), (\"bob\",)]\n",
        "df = spark.createDataFrame(data, [\"name\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unuDUtgo6MIX",
        "outputId": "59d5da52-e699-4022-b4a7-ed2c0a68a758"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+\n",
            "| name|\n",
            "+-----+\n",
            "| john|\n",
            "|alice|\n",
            "|  bob|\n",
            "+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "df.select(F.length(\"name\")).show()\n",
        "df.withColumn(\"length\", F.length(\"name\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGTaQ_kf6R0i",
        "outputId": "d2a23fdc-138d-4f46-96bf-876354599022"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|length(name)|\n",
            "+------------+\n",
            "|           4|\n",
            "|           5|\n",
            "|           3|\n",
            "+------------+\n",
            "\n",
            "+-----+------+\n",
            "| name|length|\n",
            "+-----+------+\n",
            "| john|     4|\n",
            "|alice|     5|\n",
            "|  bob|     3|\n",
            "+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **20 How to compute difference of differences between consecutive numbers of a column?**"
      ],
      "metadata": {
        "id": "IwZ0jArI8_eZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For the sake of example, we'll create a sample DataFrame\n",
        "data = [('James', 34, 55000),\n",
        "('Michael', 30, 70000),\n",
        "('Robert', 37, 60000),\n",
        "('Maria', 29, 80000),\n",
        "('Jen', 32, 65000)]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\" , \"salary\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oL7T0SC9B66",
        "outputId": "b32a067f-a658-4228-c448-345f4ea3f18b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+\n",
            "|   name|age|salary|\n",
            "+-------+---+------+\n",
            "|  James| 34| 55000|\n",
            "|Michael| 30| 70000|\n",
            "| Robert| 37| 60000|\n",
            "|  Maria| 29| 80000|\n",
            "|    Jen| 32| 65000|\n",
            "+-------+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "df = df.withColumn(\"id\", F.monotonically_increasing_id())\n",
        "window = Window.orderBy(\"id\")\n",
        "df = df.withColumn(\"data\", F.lag(F.col(\"salary\"), 1, 0).over(window))\n",
        "df = df.withColumn(\"data\", F.col(\"salary\") - F.lag(F.col(\"salary\"), 1, 0).over(window))\n",
        "df= df.drop(\"id\")\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMENQXGo9JYc",
        "outputId": "51c380c2-29f2-48a7-cb0e-d245301103a6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+------+----------+------+\n",
            "|   name|age|salary|  data|prev_value|  diff|\n",
            "+-------+---+------+------+----------+------+\n",
            "|  James| 34| 55000| 55000|      null|     0|\n",
            "|Michael| 30| 70000| 15000|     55000| 15000|\n",
            "| Robert| 37| 60000|-10000|     70000|-10000|\n",
            "|  Maria| 29| 80000| 20000|     60000| 20000|\n",
            "|    Jen| 32| 65000|-15000|     80000|-15000|\n",
            "+-------+---+------+------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Define window specification\n",
        "df = df.withColumn(\"id\", F.monotonically_increasing_id())\n",
        "window = Window.orderBy(\"id\")\n",
        "\n",
        "# Generate the lag of the variable\n",
        "df = df.withColumn(\"prev_value\", F.lag(df.salary).over(window))\n",
        "\n",
        "# Compute the difference with lag\n",
        "df = df.withColumn(\"diff\", F.when(F.isnull(df.salary - df.prev_value), 0)\n",
        ".otherwise(df.salary - df.prev_value)).drop(\"id\")\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMc9py049IU0",
        "outputId": "a7755638-3581-420b-df95-fb30f6553ea7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+------+----------+------+\n",
            "|   name|age|salary|  data|prev_value|  diff|\n",
            "+-------+---+------+------+----------+------+\n",
            "|  James| 34| 55000| 55000|      null|     0|\n",
            "|Michael| 30| 70000| 15000|     55000| 15000|\n",
            "| Robert| 37| 60000|-10000|     70000|-10000|\n",
            "|  Maria| 29| 80000| 20000|     60000| 20000|\n",
            "|    Jen| 32| 65000|-15000|     80000|-15000|\n",
            "+-------+---+------+------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **21. How to get the day of month, week number, day of year and day of week from a date strings?**"
      ],
      "metadata": {
        "id": "O5oqj0SR_swL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example data\n",
        "data = [(\"2023-05-18\",\"01 Jan 2010\",), (\"2023-12-31\", \"01 Jan 2010\",)]\n",
        "df = spark.createDataFrame(data, [\"date_str_1\", \"date_str_2\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUcfcdPv_qxo",
        "outputId": "01cd73de-9f45-47d3-8a77-d005bf3c7da9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "|date_str_1| date_str_2|\n",
            "+----------+-----------+\n",
            "|2023-05-18|01 Jan 2010|\n",
            "|2023-12-31|01 Jan 2010|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date, dayofmonth, weekofyear, dayofyear, dayofweek\n",
        "\n",
        "# Assuming df is your DataFrame with the date columns\n",
        "df_updated = df.withColumn(\"dayofmonth_1\", dayofmonth(to_date(df.date_str_1))) \\\n",
        "               .withColumn(\"weekofyear_1\", weekofyear(to_date(df.date_str_1))) \\\n",
        "               .withColumn(\"dayofyear_1\", dayofyear(to_date(df.date_str_1))) \\\n",
        "               .withColumn(\"dayofweek_1\", dayofweek(to_date(df.date_str_1))) \\\n",
        "               .withColumn(\"dayofmonth_2\", dayofmonth(to_date(df.date_str_2, 'dd MMM yyyy'))) \\\n",
        "               .withColumn(\"weekofyear_2\", weekofyear(to_date(df.date_str_2, 'dd MMM yyyy'))) \\\n",
        "               .withColumn(\"dayofyear_2\", dayofyear(to_date(df.date_str_2, 'dd MMM yyyy'))) \\\n",
        "               .withColumn(\"dayofweek_2\", dayofweek(to_date(df.date_str_2, 'dd MMM yyyy')))\n",
        "df_updated.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXd0aPHUWcoC",
        "outputId": "d4dc33e3-4eb9-4342-e647-32724b9b6a9e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+------------+------------+-----------+-----------+------------+------------+-----------+-----------+\n",
            "|date_str_1| date_str_2|dayofmonth_1|weekofyear_1|dayofyear_1|dayofweek_1|dayofmonth_2|weekofyear_2|dayofyear_2|dayofweek_2|\n",
            "+----------+-----------+------------+------------+-----------+-----------+------------+------------+-----------+-----------+\n",
            "|2023-05-18|01 Jan 2010|          18|          20|        138|          5|           1|          53|          1|          6|\n",
            "|2023-12-31|01 Jan 2010|          31|          52|        365|          1|           1|          53|          1|          6|\n",
            "+----------+-----------+------------+------------+-----------+-----------+------------+------------+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **22. How to convert year-month string to dates corresponding to the 4th day of the month?**"
      ],
      "metadata": {
        "id": "BOodVH80ZPDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example dataframe\n",
        "df = spark.createDataFrame([('Jan 2010',), ('Feb 2011',), ('Mar 2012',)], ['MonthYear'])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-aZq723ZMZF",
        "outputId": "2387f4a5-4349-46bf-c9a2-cc59dd7f2586"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|MonthYear|\n",
            "+---------+\n",
            "| Jan 2010|\n",
            "| Feb 2011|\n",
            "| Mar 2012|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import expr, col\n",
        "\n",
        "# convert YearMonth to date (default to first day of the month)\n",
        "df = df.withColumn('Date', expr(\"to_date(MonthYear, 'MMM yyyy')\"))\n",
        "\n",
        "df.show()\n",
        "\n",
        "df = df.withColumn('Date', expr(\"date_add(date_sub(Date, day(Date) - 1), 3)\"))\n",
        "\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5FXZv4aZa07",
        "outputId": "abc3f40f-13ce-4ba1-c27c-a03f2c917b7c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+\n",
            "|MonthYear|      Date|\n",
            "+---------+----------+\n",
            "| Jan 2010|2010-01-01|\n",
            "| Feb 2011|2011-02-01|\n",
            "| Mar 2012|2012-03-01|\n",
            "+---------+----------+\n",
            "\n",
            "+---------+----------+\n",
            "|MonthYear|      Date|\n",
            "+---------+----------+\n",
            "| Jan 2010|2010-01-04|\n",
            "| Feb 2011|2011-02-04|\n",
            "| Mar 2012|2012-03-04|\n",
            "+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **23 How to filter words that contain atleast 2 vowels from a series?**"
      ],
      "metadata": {
        "id": "DOuioBnRHohM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame([('Apple',), ('Orange',), ('Plan',) , ('Python',) , ('Money',)], ['Word'])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-6O60EvHoDf",
        "outputId": "43749a92-e768-487f-9226-4b99c69e6051"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|  Word|\n",
            "+------+\n",
            "| Apple|\n",
            "|Orange|\n",
            "|  Plan|\n",
            "|Python|\n",
            "| Money|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, length, translate\n",
        "\n",
        "# Filter words that contain at least 2 vowels\n",
        "df_filtered = df.where((length(col('Word')) - length(translate(col('Word'), 'AEIOUaeiou', ''))) >= 2)\n",
        "df_filtered.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxxqQh5aHxe_",
        "outputId": "94f13e40-654a-4291-e6d5-c6162b771b9e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|  Word|\n",
            "+------+\n",
            "| Apple|\n",
            "|Orange|\n",
            "| Money|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **24. How to filter valid emails from a list?**"
      ],
      "metadata": {
        "id": "p47DbY5HLx3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list\n",
        "data = ['buying books at amazom.com', 'rameses@egypt.com', 'matt@t.co', 'narendra@modi.com']\n",
        "\n",
        "# Convert the list to DataFrame\n",
        "df = spark.createDataFrame(data, \"string\")\n",
        "df.show(truncate =False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaPDZMb4Lxjw",
        "outputId": "3f3129fe-c215-470f-bcd9-92d26dcaed19"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------+\n",
            "|value                     |\n",
            "+--------------------------+\n",
            "|buying books at amazom.com|\n",
            "|rameses@egypt.com         |\n",
            "|matt@t.co                 |\n",
            "|narendra@modi.com         |\n",
            "+--------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "pattern =\"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$\"\n",
        "df_filtered = df.filter(F.col('value').rlike(pattern))\n",
        "df_filtered.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tedy9HIHMq1O",
        "outputId": "f6971dc1-76fd-4458-ae6b-30e6a913e135"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+\n",
            "|            value|\n",
            "+-----------------+\n",
            "|rameses@egypt.com|\n",
            "|        matt@t.co|\n",
            "|narendra@modi.com|\n",
            "+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **25. How to Pivot PySpark DataFrame?**\n"
      ],
      "metadata": {
        "id": "k7lu0Oa9OHzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "data = [\n",
        "(2021, 1, \"US\", 5000),\n",
        "(2021, 1, \"EU\", 4000),\n",
        "(2021, 2, \"US\", 5500),\n",
        "(2021, 2, \"EU\", 4500),\n",
        "(2021, 3, \"US\", 6000),\n",
        "(2021, 3, \"EU\", 5000),\n",
        "(2021, 4, \"US\", 7000),\n",
        "(2021, 4, \"EU\", 6000),\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "columns = [\"year\", \"quarter\", \"region\", \"revenue\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWUlumhDOFmf",
        "outputId": "2cb756c8-efc4-4d6e-fbd8-58a1235491ba"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+------+-------+\n",
            "|year|quarter|region|revenue|\n",
            "+----+-------+------+-------+\n",
            "|2021|      1|    US|   5000|\n",
            "|2021|      1|    EU|   4000|\n",
            "|2021|      2|    US|   5500|\n",
            "|2021|      2|    EU|   4500|\n",
            "|2021|      3|    US|   6000|\n",
            "|2021|      3|    EU|   5000|\n",
            "|2021|      4|    US|   7000|\n",
            "|2021|      4|    EU|   6000|\n",
            "+----+-------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "pivot_df = df.groupBy(\"year\", \"quarter\").pivot(\"region\").sum(\"revenue\")\n",
        "pivot_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DG-uPo6jObl8",
        "outputId": "f44f40be-3167-4fae-fec7-762531c66c02"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+----+----+\n",
            "|year|quarter|  EU|  US|\n",
            "+----+-------+----+----+\n",
            "|2021|      2|4500|5500|\n",
            "|2021|      1|4000|5000|\n",
            "|2021|      4|6000|7000|\n",
            "|2021|      3|5000|6000|\n",
            "+----+-------+----+----+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}