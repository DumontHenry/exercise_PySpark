{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8Wj3PQfMy5crfk0AXryFk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DumontHenry/exercise_PySpark/blob/main/PySpark_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaFYleaw-N0g",
        "outputId": "f021e7c9-8037-405d-df45-7e2375ec3dda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark==3.4.1 in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.4.1) (0.10.9.7)\n",
            "Collecting findspark==2.0.1\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark==3.4.1\n",
        "!pip install findspark==2.0.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"PySpark 101 Exercises\").getOrCreate()\n",
        "print(spark.version)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEVft8f4-eMN",
        "outputId": "5e8b9188-714c-4c0a-c7c3-1e27ddb9683e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame([\n",
        "(\"Alice\", 1),\n",
        "(\"Bob\", 2),\n",
        "(\"Charlie\", 3),\n",
        "], [\"Name\", \"Value\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLuNXZcL_6eQ",
        "outputId": "302128a9-a2be-4d8f-f24e-d5cf35031d32"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|   Name|Value|\n",
            "+-------+-----+\n",
            "|  Alice|    1|\n",
            "|    Bob|    2|\n",
            "|Charlie|    3|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Window\n",
        "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
        "\n",
        "# Applying orderBy() and monotonically_increasing_id()\n",
        "window_spec = Window.orderBy(monotonically_increasing_id())\n",
        "\n",
        "# Add a new column \"row_number\" using row_number() over the specified window\n",
        "result_df = df.withColumn(\"Index\", row_number().over(window_spec)-1)\n",
        "\n",
        "# Show the result\n",
        "result_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vz7Sb3azAW15",
        "outputId": "0fbe506b-89b4-4f16-e937-68aed7e6ad45"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+-----+\n",
            "|   Name|Value|Index|\n",
            "+-------+-----+-----+\n",
            "|  Alice|    1|    0|\n",
            "|    Bob|    2|    1|\n",
            "|Charlie|    3|    2|\n",
            "+-------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list1 = [\"a\", \"b\", \"c\", \"d\"]\n",
        "list2 = [1, 2, 3, 4]"
      ],
      "metadata": {
        "id": "ifhS1jxzD7N7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an RDD from the lists and convert it to a DataFrame\n",
        "rdd = spark.sparkContext.parallelize(list(zip(list1, list2)))\n",
        "df = rdd.toDF([\"Column1\", \"Column2\"])\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR7iUybmBqpN",
        "outputId": "a61b9986-4327-499c-8dc2-5b879d3ccd6d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+\n",
            "|Column1|Column2|\n",
            "+-------+-------+\n",
            "|      a|      1|\n",
            "|      b|      2|\n",
            "|      c|      3|\n",
            "|      d|      4|\n",
            "+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_A = [1, 2, 3, 4, 5]\n",
        "list_B = [4, 5, 6, 7, 8]"
      ],
      "metadata": {
        "id": "myd4uB28Gcys"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = spark.sparkContext\n",
        "\n",
        "rdd_A = sc.parallelize(list_A)\n",
        "rdd_B = sc.parallelize(list_B)\n",
        "\n",
        "result_rdd = rdd_A.subtract(rdd_B)\n",
        "# Collect result\n",
        "result_list = result_rdd.collect()\n",
        "print(result_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i_Yv5ebGd20",
        "outputId": "dcd371da-9ecf-4392-9b9e-30a0f7115114"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_A = [1, 2, 3, 4, 5]\n",
        "list_B = [4, 5, 6, 7, 8]"
      ],
      "metadata": {
        "id": "HovLV-mZG2z4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = spark.sparkContext\n",
        "\n",
        "rdd_A = sc.parallelize(list_A)\n",
        "rdd_B = sc.parallelize(list_B)\n",
        "\n",
        "result_rdd_A = rdd_A.subtract(rdd_B)\n",
        "result_rdd_B = rdd_B.subtract(rdd_A)\n",
        "\n",
        "result_rdd = result_rdd_A.union(result_rdd_B)\n",
        "# Collect result\n",
        "result_list = result_rdd.collect()\n",
        "print(result_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yA-F1aOROjnn",
        "outputId": "efb122bb-0b99-4d3d-ada8-8075b6b8bb74"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 8, 6, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"A\", 10), (\"B\", 20), (\"C\", 30), (\"D\", 40), (\"E\", 50), (\"F\", 15), (\"G\", 28), (\"H\", 54), (\"I\", 41), (\"J\", 86)]\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIVZ4CD6PEkZ",
        "outputId": "66a4d7fb-3fbf-42f7-fd2f-4cb11b0241ea"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+\n",
            "|Name|Age|\n",
            "+----+---+\n",
            "|   A| 10|\n",
            "|   B| 20|\n",
            "|   C| 30|\n",
            "|   D| 40|\n",
            "|   E| 50|\n",
            "|   F| 15|\n",
            "|   G| 28|\n",
            "|   H| 54|\n",
            "|   I| 41|\n",
            "|   J| 86|\n",
            "+----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantiles = df.approxQuantile(\"Age\", [0.0, 0.25, 0.5, 0.75, 1.0], 0.01)\n",
        "print(quantiles)\n",
        "print(\"Min: \", quantiles[0])\n",
        "print(\"25th percentile: \", quantiles[1])\n",
        "print(\"Median: \", quantiles[2])\n",
        "print(\"75th percentile: \", quantiles[3])\n",
        "print(\"Max: \", quantiles[4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3S7qHzhPH02",
        "outputId": "2c2b8e7d-dc56-428a-baa8-e87e0349e5c4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10.0, 20.0, 30.0, 50.0, 86.0]\n",
            "Min:  10.0\n",
            "25th percentile:  20.0\n",
            "Median:  30.0\n",
            "75th percentile:  50.0\n",
            "Max:  86.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "Row(name='John', job='Engineer'),\n",
        "Row(name='John', job='Engineer'),\n",
        "Row(name='Mary', job='Scientist'),\n",
        "Row(name='Bob', job='Engineer'),\n",
        "Row(name='Bob', job='Engineer'),\n",
        "Row(name='Bob', job='Scientist'),\n",
        "Row(name='Sam', job='Doctor'),\n",
        "]\n",
        "\n",
        "# create DataFrame\n",
        "df = spark.createDataFrame(data)\n",
        "\n",
        "# show DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYI3iN3TPkGc",
        "outputId": "b8d6b4e6-4b9b-462e-f388-6e8d250b0e7c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "|name|      job|\n",
            "+----+---------+\n",
            "|John| Engineer|\n",
            "|John| Engineer|\n",
            "|Mary|Scientist|\n",
            "| Bob| Engineer|\n",
            "| Bob| Engineer|\n",
            "| Bob|Scientist|\n",
            "| Sam|   Doctor|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select('name', 'job').distinct().collect()\n",
        "####\n",
        "df.groupby('job').count().show()\n",
        "df.groupby('name').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxgGKhHYPmuf",
        "outputId": "f8bd458d-eaa5-41c2-90f3-7d82b81ebfff"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----+\n",
            "|      job|count|\n",
            "+---------+-----+\n",
            "|Scientist|    2|\n",
            "| Engineer|    4|\n",
            "|   Doctor|    1|\n",
            "+---------+-----+\n",
            "\n",
            "+----+-----+\n",
            "|name|count|\n",
            "+----+-----+\n",
            "|Mary|    1|\n",
            "|John|    2|\n",
            "| Bob|    3|\n",
            "| Sam|    1|\n",
            "+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "Row(name='John', job='Engineer'),\n",
        "Row(name='John', job='Engineer'),\n",
        "Row(name='Mary', job='Scientist'),\n",
        "Row(name='Bob', job='Engineer'),\n",
        "Row(name='Bob', job='Engineer'),\n",
        "Row(name='Bob', job='Scientist'),\n",
        "Row(name='Sam', job='Doctor'),\n",
        "]\n",
        "\n",
        "# create DataFrame\n",
        "df = spark.createDataFrame(data)\n",
        "\n",
        "# show DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bl_r_E6nRAdB",
        "outputId": "2c45cdd0-bb19-4fe4-fb3e-ccb64ee36cb6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "|name|      job|\n",
            "+----+---------+\n",
            "|John| Engineer|\n",
            "|John| Engineer|\n",
            "|Mary|Scientist|\n",
            "| Bob| Engineer|\n",
            "| Bob| Engineer|\n",
            "| Bob|Scientist|\n",
            "| Sam|   Doctor|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "# Get the top 2 most frequent jobs\n",
        "top_2_jobs= df.groupby('job').count().orderBy('count', ascending=False).limit(2).select('job').rdd.flatMap(lambda x: x).collect()\n",
        "# Replace all but the top 2 most frequent jobs with 'Other'\n",
        "df = df.withColumn('job', when(col('job').isin(top_2_jobs), col('job')).otherwise('Other'))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7l1uKN_RHzv",
        "outputId": "77fa59d4-aab8-4287-cb60-6e866a126014"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "|name|      job|\n",
            "+----+---------+\n",
            "|John| Engineer|\n",
            "|John| Engineer|\n",
            "|Mary|Scientist|\n",
            "| Bob| Engineer|\n",
            "| Bob| Engineer|\n",
            "| Bob|Scientist|\n",
            "| Sam|    Other|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df is your DataFrame\n",
        "df = spark.createDataFrame([\n",
        "(\"A\", 1, None),\n",
        "(\"B\", None, \"123\" ),\n",
        "(\"B\", 3, \"456\"),\n",
        "(\"D\", None, None),\n",
        "], [\"Name\", \"Value\", \"id\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrFWQe9DSk4o",
        "outputId": "115dfcc8-90f4-4b85-c1f2-d72da5eab61c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+----+\n",
            "|Name|Value|  id|\n",
            "+----+-----+----+\n",
            "|   A|    1|null|\n",
            "|   B| null| 123|\n",
            "|   B|    3| 456|\n",
            "|   D| null|null|\n",
            "+----+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_1 = df.dropna(subset=['Value'], how='all')\n",
        "df_1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sfahV27TEsO",
        "outputId": "78f93357-b2d9-45f6-a8d4-0aa1c3c18f5a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+----+\n",
            "|Name|Value|  id|\n",
            "+----+-----+----+\n",
            "|   A|    1|null|\n",
            "|   B|    3| 456|\n",
            "+----+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# suppose you have the following DataFrame\n",
        "df = spark.createDataFrame([(1, 2, 3), (4, 5, 6)], [\"col1\", \"col2\", \"col3\"])\n",
        "\n",
        "# old column names\n",
        "old_names = [\"col1\", \"col2\", \"col3\"]\n",
        "\n",
        "# new column names\n",
        "new_names = [\"new_col1\", \"new_col2\", \"new_col3\"]\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuvJjLIIUOZ3",
        "outputId": "b9526e8f-ad83-4f85-e4d3-5a7acb2ce2d5"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+\n",
            "|col1|col2|col3|\n",
            "+----+----+----+\n",
            "|   1|   2|   3|\n",
            "|   4|   5|   6|\n",
            "+----+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for old_name, new_name in zip(old_names, new_names):\n",
        "  df = df.withColumnRenamed(old_name, new_name )\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvYqVS9uUgdT",
        "outputId": "32a9adfa-8bb1-47bc-e2db-9111c7a3e32a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+--------+\n",
            "|new_col1|new_col2|new_col3|\n",
            "+--------+--------+--------+\n",
            "|       1|       2|       3|\n",
            "|       4|       5|       6|\n",
            "+--------+--------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rand\n",
        "from pyspark.ml.feature import Bucketizer\n",
        "\n",
        "# Create a DataFrame with a single column \"values\" filled with random numbers\n",
        "num_items = 100\n",
        "df = spark.range(num_items).select(rand(seed=42).alias(\"values\"))\n",
        "\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bB6g0gfWUw98",
        "outputId": "3f7228a0-d455-47c5-fa14-5544b81fc300"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "|             values|\n",
            "+-------------------+\n",
            "|  0.619189370225301|\n",
            "| 0.5096018842446481|\n",
            "| 0.8325259388871524|\n",
            "|0.26322809041172357|\n",
            "| 0.6702867696264135|\n",
            "+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_buckets = 10  # Number of buckets\n",
        "quantiles = df.stat.approxQuantile(\"values\",[i/num_buckets for i in range(num_buckets+1)], 0.01)\n",
        "\n",
        "# Create the Bucketizer\n",
        "bucketizer = Bucketizer(splits=quantiles, inputCol=\"values\", outputCol=\"buckets\")\n",
        "\n",
        "# Apply the Bucketizer\n",
        "df_buck = bucketizer.transform(df)\n",
        "\n",
        "#Frequency table\n",
        "df_buck.groupBy(\"buckets\").count().show()\n",
        "\n",
        "# Show the original and bucketed values\n",
        "df_buck.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-khphvLWDD8",
        "outputId": "6c1204a9-b0b4-432b-a7be-877dcdf42129"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|buckets|count|\n",
            "+-------+-----+\n",
            "|    8.0|   10|\n",
            "|    0.0|    8|\n",
            "|    7.0|   10|\n",
            "|    1.0|   10|\n",
            "|    4.0|   10|\n",
            "|    3.0|   10|\n",
            "|    2.0|   10|\n",
            "|    6.0|   10|\n",
            "|    5.0|   10|\n",
            "|    9.0|   12|\n",
            "+-------+-----+\n",
            "\n",
            "+-------------------+-------+\n",
            "|             values|buckets|\n",
            "+-------------------+-------+\n",
            "|  0.619189370225301|    4.0|\n",
            "| 0.5096018842446481|    4.0|\n",
            "| 0.8325259388871524|    8.0|\n",
            "|0.26322809041172357|    2.0|\n",
            "| 0.6702867696264135|    5.0|\n",
            "+-------------------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example DataFrame\n",
        "data = [(\"A\", \"X\"), (\"A\", \"Y\"), (\"A\", \"X\"), (\"B\", \"Y\"), (\"B\", \"X\"), (\"C\", \"X\"), (\"C\", \"X\"), (\"C\", \"Y\")]\n",
        "df = spark.createDataFrame(data, [\"category1\", \"category2\"])\n",
        "\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmg3shZGWeJh",
        "outputId": "b465b152-f17c-4237-cd35-8f55494e0028"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+\n",
            "|category1|category2|\n",
            "+---------+---------+\n",
            "|        A|        X|\n",
            "|        A|        Y|\n",
            "|        A|        X|\n",
            "|        B|        Y|\n",
            "|        B|        X|\n",
            "|        C|        X|\n",
            "|        C|        X|\n",
            "|        C|        Y|\n",
            "+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.cube(\"category1\").count().show()\n",
        "\"\"\"\n",
        "The cube function in PySpark is used to perform data aggregation by generating all possible combinations of grouping columns, similar to how the groupby function works.\n",
        "However, cube goes a step further by also including aggregations for all subsets of the specified grouping columns, as well as the grand total (aggregation across all data).\n",
        "This means that if you apply cube on multiple columns, it will generate aggregations for each individual column, all possible pairs, all possible triplets, and so on, along with the overall total.\n",
        "For example, in the code you provided, df.cube(\"category1\").count().show(), the cube function is applied to the \"category1\" column.\n",
        "This will generate counts for each distinct value in \"category1\" as well as a total count for all categories combined.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnDem-E6Wg23",
        "outputId": "12b18703-951e-4665-fafe-a7dbb4d67b70"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----+\n",
            "|category1|count|\n",
            "+---------+-----+\n",
            "|        B|    2|\n",
            "|     null|    8|\n",
            "|        A|    3|\n",
            "|        C|    3|\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Contingency table\n",
        "df.crosstab('category1', 'category2').show()\n",
        "\"\"\"\n",
        "The crosstab function in PySpark is used to compute a contingency table (also known as a cross-tabulation) for two columns of a DataFrame.\n",
        " It essentially creates a frequency table that shows the distribution of values in one column, categorized by the values in another column.\n",
        "In your code, df.crosstab('category1', 'category2').show(), it generates a table that shows how many times each combination of values from \"category1\" and \"category2\" appears in your DataFrame.\n",
        "This allows you to see the relationship or association between those two categorical variables\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQLTaMRVXXMy",
        "outputId": "c6f8f9e5-a228-454e-ecda-19e4fe9fcbf5"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+---+---+\n",
            "|category1_category2|  X|  Y|\n",
            "+-------------------+---+---+\n",
            "|                  B|  1|  1|\n",
            "|                  C|  2|  1|\n",
            "|                  A|  2|  1|\n",
            "+-------------------+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rand\n",
        "\n",
        "# Generate a DataFrame with a single column \"id\" with 10 rows\n",
        "df = spark.range(10)\n",
        "# Generate a random float between 0 and 1, scale and shift it to get a random integer between 1 and 10\n",
        "df = df.withColumn(\"random\", ((rand(seed=42) * 10) + 1).cast(\"int\"))\n",
        "# Show the DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyVru8o1X2Hn",
        "outputId": "fca48dcd-911a-437e-a347-39bab9138c17"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| id|random|\n",
            "+---+------+\n",
            "|  0|     7|\n",
            "|  1|     6|\n",
            "|  2|     9|\n",
            "|  3|     3|\n",
            "|  4|     7|\n",
            "|  5|     9|\n",
            "|  6|     7|\n",
            "|  7|     3|\n",
            "|  8|     3|\n",
            "|  9|     7|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "df = df.withColumn(\"is_multiple_of_3\", when(col(\"random\")%3==0, 1).otherwise(0))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4lA7MyRZBr_",
        "outputId": "fc2d7313-bc84-4e48-eb1b-52f6bbf6879c"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+----------------+\n",
            "| id|random|is_multiple_of_3|\n",
            "+---+------+----------------+\n",
            "|  0|     7|               0|\n",
            "|  1|     6|               1|\n",
            "|  2|     9|               1|\n",
            "|  3|     3|               1|\n",
            "|  4|     7|               0|\n",
            "|  5|     9|               1|\n",
            "|  6|     7|               0|\n",
            "|  7|     3|               1|\n",
            "|  8|     3|               1|\n",
            "|  9|     7|               0|\n",
            "+---+------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rand\n",
        "\n",
        "# Generate a DataFrame with a single column \"id\" with 10 rows\n",
        "df = spark.range(10)\n",
        "\n",
        "# Generate a random float between 0 and 1, scale and shift it to get a random integer between 1 and 10\n",
        "df = df.withColumn(\"random\", ((rand(seed=42) * 10) + 1).cast(\"int\"))\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()\n",
        "\n",
        "pos = [0, 4, 8, 5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmOT-afiaV4H",
        "outputId": "3f1a7cf7-d8cf-4903-bf0d-13add16462d2"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| id|random|\n",
            "+---+------+\n",
            "|  0|     7|\n",
            "|  1|     6|\n",
            "|  2|     9|\n",
            "|  3|     3|\n",
            "|  4|     7|\n",
            "|  5|     9|\n",
            "|  6|     7|\n",
            "|  7|     3|\n",
            "|  8|     3|\n",
            "|  9|     7|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos = [0, 4, 8, 5]\n",
        "\n",
        "# Define window specification\n",
        "w = Window.orderBy(monotonically_increasing_id())\n",
        "\n",
        "# Add index\n",
        "df = df.withColumn(\"index\", row_number().over(w) - 1)\n",
        "\n",
        "df.show()\n",
        "\n",
        "# Filter the DataFrame based on the specified positions\n",
        "df_filtered = df.filter(df.index.isin(pos))\n",
        "\n",
        "df_filtered.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImR9hKuwbdnq",
        "outputId": "7f76db72-8b57-40cb-92ae-6ef570aa688a"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+-----+\n",
            "| id|random|index|\n",
            "+---+------+-----+\n",
            "|  0|     7|    0|\n",
            "|  1|     6|    1|\n",
            "|  2|     9|    2|\n",
            "|  3|     3|    3|\n",
            "|  4|     7|    4|\n",
            "|  5|     9|    5|\n",
            "|  6|     7|    6|\n",
            "|  7|     3|    7|\n",
            "|  8|     3|    8|\n",
            "|  9|     7|    9|\n",
            "+---+------+-----+\n",
            "\n",
            "+---+------+-----+\n",
            "| id|random|index|\n",
            "+---+------+-----+\n",
            "|  0|     7|    0|\n",
            "|  4|     7|    4|\n",
            "|  5|     9|    5|\n",
            "|  8|     3|    8|\n",
            "+---+------+-----+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}