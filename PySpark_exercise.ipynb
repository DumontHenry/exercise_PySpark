{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDCBA6DLi8VsdVqO42bHHU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DumontHenry/exercise_PySpark/blob/main/PySpark_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaFYleaw-N0g",
        "outputId": "3b5daa00-4bc9-4aaa-9be0-846e5ffb86f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.4.1\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.4.1) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285386 sha256=5c261d62fadde1757d87d69c80cd6839aa0defe8dc870ed95a70e08245465e85\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n",
            "Collecting findspark==2.0.1\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark==3.4.1\n",
        "!pip install findspark==2.0.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"PySpark 101 Exercises\").getOrCreate()\n",
        "print(spark.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEVft8f4-eMN",
        "outputId": "070584d8-fbe5-445e-c739-b76807468367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# https://sparkbyexamples.com/\n",
        "**example pyspark code**\n",
        "\n",
        "\n",
        "#https://www.machinelearningplus.com/pyspark/introduction-to-pyspark/\n",
        "**Introduction to pyspark**"
      ],
      "metadata": {
        "id": "sR1fzFItxCRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# withColumn = By using PySpark withColumn() on a DataFrame, we can cast or change the data type of a column.\n",
        "# withColumn =  PySpark withColumn() function of DataFrame can also be used to change the value of an existing column.$\n",
        "# withColumn = To add/create a new column, specify the first argument with a name you want your new column to be and use the second argument to assign a value by applying an operation on an existing column\n",
        "# viwthColumn = In order to create a new column, pass the column name you wanted to the first argument of withColumn() transformation function.\n",
        "\n",
        "\n",
        "# withColumnRenamed() = Though you cannot rename a column using withColumn, still I wanted to cover this as renaming is one of the common operations we perform on DataFrame. To rename an existing column use withColumnRenamed() function on DataFrame.\n",
        "#  SparkSession.builder.appName('SparkByExamples.com').getOrCreate() = In PySpark, SparkSession.builder.appName() sets a name for your Spark application. This name is useful for identifying your application in Spark's web UI or logs"
      ],
      "metadata": {
        "id": "kMJ0-iEO66gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame([\n",
        "(\"Alice\", 1),\n",
        "(\"Bob\", 2),\n",
        "(\"Charlie\", 3),\n",
        "], [\"Name\", \"Value\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLuNXZcL_6eQ",
        "outputId": "5fe1274b-541e-4690-d5f5-edeec0660c8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|   Name|Value|\n",
            "+-------+-----+\n",
            "|  Alice|    1|\n",
            "|    Bob|    2|\n",
            "|Charlie|    3|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Window\n",
        "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
        "\n",
        "# Applying orderBy() and monotonically_increasing_id()\n",
        "window_spec = Window.orderBy(monotonically_increasing_id())\n",
        "\n",
        "# Add a new column \"row_number\" using row_number() over the specified window\n",
        "result_df = df.withColumn(\"Index\", row_number().over(window_spec)-1)\n",
        "\n",
        "# Show the result\n",
        "result_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vz7Sb3azAW15",
        "outputId": "5b523f84-a293-4cdf-8845-c36cf45cd37b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+-----+\n",
            "|   Name|Value|Index|\n",
            "+-------+-----+-----+\n",
            "|  Alice|    1|    0|\n",
            "|    Bob|    2|    1|\n",
            "|Charlie|    3|    2|\n",
            "+-------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list1 = [\"a\", \"b\", \"c\", \"d\"]\n",
        "list2 = [1, 2, 3, 4]"
      ],
      "metadata": {
        "id": "ifhS1jxzD7N7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an RDD from the lists and convert it to a DataFrame\n",
        "rdd = spark.sparkContext.parallelize(list(zip(list1, list2)))\n",
        "df = rdd.toDF([\"Column1\", \"Column2\"])\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR7iUybmBqpN",
        "outputId": "a61b9986-4327-499c-8dc2-5b879d3ccd6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+\n",
            "|Column1|Column2|\n",
            "+-------+-------+\n",
            "|      a|      1|\n",
            "|      b|      2|\n",
            "|      c|      3|\n",
            "|      d|      4|\n",
            "+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_A = [1, 2, 3, 4, 5]\n",
        "list_B = [4, 5, 6, 7, 8]"
      ],
      "metadata": {
        "id": "myd4uB28Gcys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = spark.sparkContext\n",
        "\n",
        "rdd_A = sc.parallelize(list_A)\n",
        "rdd_B = sc.parallelize(list_B)\n",
        "\n",
        "result_rdd = rdd_A.subtract(rdd_B)\n",
        "# Collect result\n",
        "result_list = result_rdd.collect()\n",
        "print(result_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i_Yv5ebGd20",
        "outputId": "dcd371da-9ecf-4392-9b9e-30a0f7115114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_A = [1, 2, 3, 4, 5]\n",
        "list_B = [4, 5, 6, 7, 8]"
      ],
      "metadata": {
        "id": "HovLV-mZG2z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = spark.sparkContext\n",
        "\n",
        "rdd_A = sc.parallelize(list_A)\n",
        "rdd_B = sc.parallelize(list_B)\n",
        "\n",
        "result_rdd_A = rdd_A.subtract(rdd_B)\n",
        "result_rdd_B = rdd_B.subtract(rdd_A)\n",
        "\n",
        "result_rdd = result_rdd_A.union(result_rdd_B)\n",
        "# Collect result\n",
        "result_list = result_rdd.collect()\n",
        "print(result_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yA-F1aOROjnn",
        "outputId": "efb122bb-0b99-4d3d-ada8-8075b6b8bb74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 8, 6, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"A\", 10), (\"B\", 20), (\"C\", 30), (\"D\", 40), (\"E\", 50), (\"F\", 15), (\"G\", 28), (\"H\", 54), (\"I\", 41), (\"J\", 86)]\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIVZ4CD6PEkZ",
        "outputId": "66a4d7fb-3fbf-42f7-fd2f-4cb11b0241ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+\n",
            "|Name|Age|\n",
            "+----+---+\n",
            "|   A| 10|\n",
            "|   B| 20|\n",
            "|   C| 30|\n",
            "|   D| 40|\n",
            "|   E| 50|\n",
            "|   F| 15|\n",
            "|   G| 28|\n",
            "|   H| 54|\n",
            "|   I| 41|\n",
            "|   J| 86|\n",
            "+----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantiles = df.approxQuantile(\"Age\", [0.0, 0.25, 0.5, 0.75, 1.0], 0.01)\n",
        "print(quantiles)\n",
        "print(\"Min: \", quantiles[0])\n",
        "print(\"25th percentile: \", quantiles[1])\n",
        "print(\"Median: \", quantiles[2])\n",
        "print(\"75th percentile: \", quantiles[3])\n",
        "print(\"Max: \", quantiles[4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3S7qHzhPH02",
        "outputId": "2c2b8e7d-dc56-428a-baa8-e87e0349e5c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10.0, 20.0, 30.0, 50.0, 86.0]\n",
            "Min:  10.0\n",
            "25th percentile:  20.0\n",
            "Median:  30.0\n",
            "75th percentile:  50.0\n",
            "Max:  86.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "Row(name='John', job='Engineer'),\n",
        "Row(name='John', job='Engineer'),\n",
        "Row(name='Mary', job='Scientist'),\n",
        "Row(name='Bob', job='Engineer'),\n",
        "Row(name='Bob', job='Engineer'),\n",
        "Row(name='Bob', job='Scientist'),\n",
        "Row(name='Sam', job='Doctor'),\n",
        "]\n",
        "\n",
        "# create DataFrame\n",
        "df = spark.createDataFrame(data)\n",
        "\n",
        "# show DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "id": "JYI3iN3TPkGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.select('name', 'job').distinct().collect()\n",
        "####\n",
        "df.groupby('job').count().show()\n",
        "df.groupby('name').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxgGKhHYPmuf",
        "outputId": "f8bd458d-eaa5-41c2-90f3-7d82b81ebfff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----+\n",
            "|      job|count|\n",
            "+---------+-----+\n",
            "|Scientist|    2|\n",
            "| Engineer|    4|\n",
            "|   Doctor|    1|\n",
            "+---------+-----+\n",
            "\n",
            "+----+-----+\n",
            "|name|count|\n",
            "+----+-----+\n",
            "|Mary|    1|\n",
            "|John|    2|\n",
            "| Bob|    3|\n",
            "| Sam|    1|\n",
            "+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "Row(name='John', job='Engineer'),\n",
        "Row(name='John', job='Engineer'),\n",
        "Row(name='Mary', job='Scientist'),\n",
        "Row(name='Bob', job='Engineer'),\n",
        "Row(name='Bob', job='Engineer'),\n",
        "Row(name='Bob', job='Scientist'),\n",
        "Row(name='Sam', job='Doctor'),\n",
        "]\n",
        "\n",
        "# create DataFrame\n",
        "df = spark.createDataFrame(data)\n",
        "\n",
        "# show DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bl_r_E6nRAdB",
        "outputId": "2c45cdd0-bb19-4fe4-fb3e-ccb64ee36cb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "|name|      job|\n",
            "+----+---------+\n",
            "|John| Engineer|\n",
            "|John| Engineer|\n",
            "|Mary|Scientist|\n",
            "| Bob| Engineer|\n",
            "| Bob| Engineer|\n",
            "| Bob|Scientist|\n",
            "| Sam|   Doctor|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "# Get the top 2 most frequent jobs\n",
        "top_2_jobs= df.groupby('job').count().orderBy('count', ascending=False).limit(2).select('job').rdd.flatMap(lambda x: x).collect()\n",
        "# Replace all but the top 2 most frequent jobs with 'Other'\n",
        "df = df.withColumn('job', when(col('job').isin(top_2_jobs), col('job')).otherwise('Other'))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7l1uKN_RHzv",
        "outputId": "77fa59d4-aab8-4287-cb60-6e866a126014"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "|name|      job|\n",
            "+----+---------+\n",
            "|John| Engineer|\n",
            "|John| Engineer|\n",
            "|Mary|Scientist|\n",
            "| Bob| Engineer|\n",
            "| Bob| Engineer|\n",
            "| Bob|Scientist|\n",
            "| Sam|    Other|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df is your DataFrame\n",
        "df = spark.createDataFrame([\n",
        "(\"A\", 1, None),\n",
        "(\"B\", None, \"123\" ),\n",
        "(\"B\", 3, \"456\"),\n",
        "(\"D\", None, None),\n",
        "], [\"Name\", \"Value\", \"id\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrFWQe9DSk4o",
        "outputId": "115dfcc8-90f4-4b85-c1f2-d72da5eab61c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+----+\n",
            "|Name|Value|  id|\n",
            "+----+-----+----+\n",
            "|   A|    1|null|\n",
            "|   B| null| 123|\n",
            "|   B|    3| 456|\n",
            "|   D| null|null|\n",
            "+----+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_1 = df.dropna(subset=['Value'], how='all')\n",
        "df_1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sfahV27TEsO",
        "outputId": "78f93357-b2d9-45f6-a8d4-0aa1c3c18f5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+----+\n",
            "|Name|Value|  id|\n",
            "+----+-----+----+\n",
            "|   A|    1|null|\n",
            "|   B|    3| 456|\n",
            "+----+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# suppose you have the following DataFrame\n",
        "df = spark.createDataFrame([(1, 2, 3), (4, 5, 6)], [\"col1\", \"col2\", \"col3\"])\n",
        "\n",
        "# old column names\n",
        "old_names = [\"col1\", \"col2\", \"col3\"]\n",
        "\n",
        "# new column names\n",
        "new_names = [\"new_col1\", \"new_col2\", \"new_col3\"]\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuvJjLIIUOZ3",
        "outputId": "b9526e8f-ad83-4f85-e4d3-5a7acb2ce2d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+\n",
            "|col1|col2|col3|\n",
            "+----+----+----+\n",
            "|   1|   2|   3|\n",
            "|   4|   5|   6|\n",
            "+----+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for old_name, new_name in zip(old_names, new_names):\n",
        "  df = df.withColumnRenamed(old_name, new_name )\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvYqVS9uUgdT",
        "outputId": "32a9adfa-8bb1-47bc-e2db-9111c7a3e32a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+--------+\n",
            "|new_col1|new_col2|new_col3|\n",
            "+--------+--------+--------+\n",
            "|       1|       2|       3|\n",
            "|       4|       5|       6|\n",
            "+--------+--------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rand\n",
        "from pyspark.ml.feature import Bucketizer\n",
        "\n",
        "# Create a DataFrame with a single column \"values\" filled with random numbers\n",
        "num_items = 100\n",
        "df = spark.range(num_items).select(rand(seed=42).alias(\"values\"))\n",
        "\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bB6g0gfWUw98",
        "outputId": "3f7228a0-d455-47c5-fa14-5544b81fc300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "|             values|\n",
            "+-------------------+\n",
            "|  0.619189370225301|\n",
            "| 0.5096018842446481|\n",
            "| 0.8325259388871524|\n",
            "|0.26322809041172357|\n",
            "| 0.6702867696264135|\n",
            "+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_buckets = 10  # Number of buckets\n",
        "quantiles = df.stat.approxQuantile(\"values\",[i/num_buckets for i in range(num_buckets+1)], 0.01)\n",
        "\n",
        "# Create the Bucketizer\n",
        "bucketizer = Bucketizer(splits=quantiles, inputCol=\"values\", outputCol=\"buckets\")\n",
        "\n",
        "# Apply the Bucketizer\n",
        "df_buck = bucketizer.transform(df)\n",
        "\n",
        "#Frequency table\n",
        "df_buck.groupBy(\"buckets\").count().show()\n",
        "\n",
        "# Show the original and bucketed values\n",
        "df_buck.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-khphvLWDD8",
        "outputId": "6c1204a9-b0b4-432b-a7be-877dcdf42129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|buckets|count|\n",
            "+-------+-----+\n",
            "|    8.0|   10|\n",
            "|    0.0|    8|\n",
            "|    7.0|   10|\n",
            "|    1.0|   10|\n",
            "|    4.0|   10|\n",
            "|    3.0|   10|\n",
            "|    2.0|   10|\n",
            "|    6.0|   10|\n",
            "|    5.0|   10|\n",
            "|    9.0|   12|\n",
            "+-------+-----+\n",
            "\n",
            "+-------------------+-------+\n",
            "|             values|buckets|\n",
            "+-------------------+-------+\n",
            "|  0.619189370225301|    4.0|\n",
            "| 0.5096018842446481|    4.0|\n",
            "| 0.8325259388871524|    8.0|\n",
            "|0.26322809041172357|    2.0|\n",
            "| 0.6702867696264135|    5.0|\n",
            "+-------------------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example DataFrame\n",
        "data = [(\"A\", \"X\"), (\"A\", \"Y\"), (\"A\", \"X\"), (\"B\", \"Y\"), (\"B\", \"X\"), (\"C\", \"X\"), (\"C\", \"X\"), (\"C\", \"Y\")]\n",
        "df = spark.createDataFrame(data, [\"category1\", \"category2\"])\n",
        "\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmg3shZGWeJh",
        "outputId": "b465b152-f17c-4237-cd35-8f55494e0028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+\n",
            "|category1|category2|\n",
            "+---------+---------+\n",
            "|        A|        X|\n",
            "|        A|        Y|\n",
            "|        A|        X|\n",
            "|        B|        Y|\n",
            "|        B|        X|\n",
            "|        C|        X|\n",
            "|        C|        X|\n",
            "|        C|        Y|\n",
            "+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.cube(\"category1\").count().show()\n",
        "\"\"\"\n",
        "The cube function in PySpark is used to perform data aggregation by generating all possible combinations of grouping columns, similar to how the groupby function works.\n",
        "However, cube goes a step further by also including aggregations for all subsets of the specified grouping columns, as well as the grand total (aggregation across all data).\n",
        "This means that if you apply cube on multiple columns, it will generate aggregations for each individual column, all possible pairs, all possible triplets, and so on, along with the overall total.\n",
        "For example, in the code you provided, df.cube(\"category1\").count().show(), the cube function is applied to the \"category1\" column.\n",
        "This will generate counts for each distinct value in \"category1\" as well as a total count for all categories combined.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnDem-E6Wg23",
        "outputId": "12b18703-951e-4665-fafe-a7dbb4d67b70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----+\n",
            "|category1|count|\n",
            "+---------+-----+\n",
            "|        B|    2|\n",
            "|     null|    8|\n",
            "|        A|    3|\n",
            "|        C|    3|\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Contingency table\n",
        "df.crosstab('category1', 'category2').show()\n",
        "\"\"\"\n",
        "The crosstab function in PySpark is used to compute a contingency table (also known as a cross-tabulation) for two columns of a DataFrame.\n",
        " It essentially creates a frequency table that shows the distribution of values in one column, categorized by the values in another column.\n",
        "In your code, df.crosstab('category1', 'category2').show(), it generates a table that shows how many times each combination of values from \"category1\" and \"category2\" appears in your DataFrame.\n",
        "This allows you to see the relationship or association between those two categorical variables\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQLTaMRVXXMy",
        "outputId": "c6f8f9e5-a228-454e-ecda-19e4fe9fcbf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+---+---+\n",
            "|category1_category2|  X|  Y|\n",
            "+-------------------+---+---+\n",
            "|                  B|  1|  1|\n",
            "|                  C|  2|  1|\n",
            "|                  A|  2|  1|\n",
            "+-------------------+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rand\n",
        "\n",
        "# Generate a DataFrame with a single column \"id\" with 10 rows\n",
        "df = spark.range(10)\n",
        "# Generate a random float between 0 and 1, scale and shift it to get a random integer between 1 and 10\n",
        "df = df.withColumn(\"random\", ((rand(seed=42) * 10) + 1).cast(\"int\"))\n",
        "# Show the DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyVru8o1X2Hn",
        "outputId": "fca48dcd-911a-437e-a347-39bab9138c17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| id|random|\n",
            "+---+------+\n",
            "|  0|     7|\n",
            "|  1|     6|\n",
            "|  2|     9|\n",
            "|  3|     3|\n",
            "|  4|     7|\n",
            "|  5|     9|\n",
            "|  6|     7|\n",
            "|  7|     3|\n",
            "|  8|     3|\n",
            "|  9|     7|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "df = df.withColumn(\"is_multiple_of_3\", when(col(\"random\")%3==0, 1).otherwise(0))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4lA7MyRZBr_",
        "outputId": "fc2d7313-bc84-4e48-eb1b-52f6bbf6879c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+----------------+\n",
            "| id|random|is_multiple_of_3|\n",
            "+---+------+----------------+\n",
            "|  0|     7|               0|\n",
            "|  1|     6|               1|\n",
            "|  2|     9|               1|\n",
            "|  3|     3|               1|\n",
            "|  4|     7|               0|\n",
            "|  5|     9|               1|\n",
            "|  6|     7|               0|\n",
            "|  7|     3|               1|\n",
            "|  8|     3|               1|\n",
            "|  9|     7|               0|\n",
            "+---+------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rand\n",
        "\n",
        "# Generate a DataFrame with a single column \"id\" with 10 rows\n",
        "df = spark.range(10)\n",
        "\n",
        "# Generate a random float between 0 and 1, scale and shift it to get a random integer between 1 and 10\n",
        "df = df.withColumn(\"random\", ((rand(seed=42) * 10) + 1).cast(\"int\"))\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()\n",
        "\n",
        "pos = [0, 4, 8, 5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmOT-afiaV4H",
        "outputId": "3f1a7cf7-d8cf-4903-bf0d-13add16462d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| id|random|\n",
            "+---+------+\n",
            "|  0|     7|\n",
            "|  1|     6|\n",
            "|  2|     9|\n",
            "|  3|     3|\n",
            "|  4|     7|\n",
            "|  5|     9|\n",
            "|  6|     7|\n",
            "|  7|     3|\n",
            "|  8|     3|\n",
            "|  9|     7|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos = [0, 4, 8, 5]\n",
        "\n",
        "# Define window specification\n",
        "w = Window.orderBy(monotonically_increasing_id())\n",
        "\n",
        "# Add index\n",
        "df = df.withColumn(\"index\", row_number().over(w) - 1)\n",
        "\n",
        "df.show()\n",
        "\n",
        "# Filter the DataFrame based on the specified positions\n",
        "df_filtered = df.filter(df.index.isin(pos))\n",
        "\n",
        "df_filtered.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImR9hKuwbdnq",
        "outputId": "7f76db72-8b57-40cb-92ae-6ef570aa688a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+-----+\n",
            "| id|random|index|\n",
            "+---+------+-----+\n",
            "|  0|     7|    0|\n",
            "|  1|     6|    1|\n",
            "|  2|     9|    2|\n",
            "|  3|     3|    3|\n",
            "|  4|     7|    4|\n",
            "|  5|     9|    5|\n",
            "|  6|     7|    6|\n",
            "|  7|     3|    7|\n",
            "|  8|     3|    8|\n",
            "|  9|     7|    9|\n",
            "+---+------+-----+\n",
            "\n",
            "+---+------+-----+\n",
            "| id|random|index|\n",
            "+---+------+-----+\n",
            "|  0|     7|    0|\n",
            "|  4|     7|    4|\n",
            "|  5|     9|    5|\n",
            "|  8|     3|    8|\n",
            "+---+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_A = spark.createDataFrame([(\"apple\", 3, 5), (\"banana\", 1, 10), (\"orange\", 2, 8)], [\"Name\", \"Col_1\", \"Col_2\"])\n",
        "df_A.show()\n",
        "\n",
        "# Create DataFrame for region B\n",
        "df_B = spark.createDataFrame([(\"apple\", 3, 5), (\"banana\", 1, 15), (\"grape\", 4, 6)], [\"Name\", \"Col_1\", \"Col_3\"])\n",
        "df_B.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDdT7sQOc9fP",
        "outputId": "56adc0a3-ee6c-456c-deee-536f7d6dc3ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-----+\n",
            "|  Name|Col_1|Col_2|\n",
            "+------+-----+-----+\n",
            "| apple|    3|    5|\n",
            "|banana|    1|   10|\n",
            "|orange|    2|    8|\n",
            "+------+-----+-----+\n",
            "\n",
            "+------+-----+-----+\n",
            "|  Name|Col_1|Col_3|\n",
            "+------+-----+-----+\n",
            "| apple|    3|    5|\n",
            "|banana|    1|   15|\n",
            "| grape|    4|    6|\n",
            "+------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df= df_A.union(df_B)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovqTLk6Ohsxo",
        "outputId": "97abc535-1648-454c-e918-387d57ed83af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-----+\n",
            "|  Name|Col_1|Col_2|\n",
            "+------+-----+-----+\n",
            "| apple|    3|    5|\n",
            "|banana|    1|   10|\n",
            "|orange|    2|    8|\n",
            "| apple|    3|    5|\n",
            "|banana|    1|   15|\n",
            "| grape|    4|    6|\n",
            "+------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume you have a DataFrame df with two columns \"actual\" and \"predicted\"\n",
        "# For the sake of example, we'll create a sample DataFrame\n",
        "data = [(1, 1), (2, 4), (3, 9), (4, 16), (5, 25)]\n",
        "df = spark.createDataFrame(data, [\"actual\", \"predicted\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MptRWLJFvgKR",
        "outputId": "d2af3265-9db2-4c16-92d3-a554427168c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+\n",
            "|actual|predicted|\n",
            "+------+---------+\n",
            "|     1|        1|\n",
            "|     2|        4|\n",
            "|     3|        9|\n",
            "|     4|       16|\n",
            "|     5|       25|\n",
            "+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when\n",
        "df= df.withColumn(\"squarred_error\", pow(col(\"actual\")-col(\"predicted\"), 2))\n",
        "mse = df.agg({\"squarred_error\": \"avg\"}).collect()[0][0]\n",
        "df.show()\n",
        "print(f\"Mean Squared Error (MSE) = {mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md_2XwnzwMCW",
        "outputId": "5272e3b6-a498-4e92-eb20-a0ba0d0836fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+--------------+\n",
            "|actual|predicted|squarred_error|\n",
            "+------+---------+--------------+\n",
            "|     1|        1|           0.0|\n",
            "|     2|        4|           4.0|\n",
            "|     3|        9|          36.0|\n",
            "|     4|       16|         144.0|\n",
            "|     5|       25|         400.0|\n",
            "+------+---------+--------------+\n",
            "\n",
            "Mean Squared Error (MSE) = 116.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **17. How to convert the first character of each element in a series to uppercase?**"
      ],
      "metadata": {
        "id": "gcFEeuCi5zEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppose you have the following DataFrame\n",
        "data = [(\"john\",), (\"alice\",), (\"bob\",)]\n",
        "df = spark.createDataFrame(data, [\"name\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcivcwM5xSmf",
        "outputId": "d311492b-b91e-4124-8ad3-71e06d21ba24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+\n",
            "| name|\n",
            "+-----+\n",
            "| john|\n",
            "|alice|\n",
            "|  bob|\n",
            "+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "df.withColumn(\"name\", initcap(col(\"name\"))).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbdhPcdwxU9O",
        "outputId": "287a960a-f66f-4fe5-dc25-9b62b138f952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+\n",
            "| name|\n",
            "+-----+\n",
            "| John|\n",
            "|Alice|\n",
            "|  Bob|\n",
            "+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **18. How to compute summary statistics for all columns in a dataframe**"
      ],
      "metadata": {
        "id": "Oy5Ik8-J5tTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For the sake of example, we'll create a sample DataFrame\n",
        "data = [('James', 34, 55000),\n",
        "('Michael', 30, 70000),\n",
        "('Robert', 37, 60000),\n",
        "('Maria', 29, 80000),\n",
        "('Jen', 32, 65000)]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\" , \"salary\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eko96r0fx_J4",
        "outputId": "5d138e17-8018-4b2e-9d2e-79ebb2e9246e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+\n",
            "|   name|age|salary|\n",
            "+-------+---+------+\n",
            "|  James| 34| 55000|\n",
            "|Michael| 30| 70000|\n",
            "| Robert| 37| 60000|\n",
            "|  Maria| 29| 80000|\n",
            "|    Jen| 32| 65000|\n",
            "+-------+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary = df.summary()\n",
        "summary.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwpPQOWa5dce",
        "outputId": "6490e5d7-9bc3-4746-fe5d-1253fed580d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+-----------------+-----------------+\n",
            "|summary|  name|              age|           salary|\n",
            "+-------+------+-----------------+-----------------+\n",
            "|  count|     5|                5|                5|\n",
            "|   mean|  null|             32.4|          66000.0|\n",
            "| stddev|  null|3.209361307176242|9617.692030835671|\n",
            "|    min| James|               29|            55000|\n",
            "|    25%|  null|               30|            60000|\n",
            "|    50%|  null|               32|            65000|\n",
            "|    75%|  null|               34|            70000|\n",
            "|    max|Robert|               37|            80000|\n",
            "+-------+------+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **19. How to calculate the number of characters in each word in a column?**"
      ],
      "metadata": {
        "id": "piaAfnkr53im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppose you have the following DataFrame\n",
        "data = [(\"john\",), (\"alice\",), (\"bob\",)]\n",
        "df = spark.createDataFrame(data, [\"name\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unuDUtgo6MIX",
        "outputId": "59d5da52-e699-4022-b4a7-ed2c0a68a758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+\n",
            "| name|\n",
            "+-----+\n",
            "| john|\n",
            "|alice|\n",
            "|  bob|\n",
            "+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "df.select(F.length(\"name\")).show()\n",
        "df.withColumn(\"length\", F.length(\"name\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGTaQ_kf6R0i",
        "outputId": "d2a23fdc-138d-4f46-96bf-876354599022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|length(name)|\n",
            "+------------+\n",
            "|           4|\n",
            "|           5|\n",
            "|           3|\n",
            "+------------+\n",
            "\n",
            "+-----+------+\n",
            "| name|length|\n",
            "+-----+------+\n",
            "| john|     4|\n",
            "|alice|     5|\n",
            "|  bob|     3|\n",
            "+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **20 How to compute difference of differences between consecutive numbers of a column?**"
      ],
      "metadata": {
        "id": "IwZ0jArI8_eZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For the sake of example, we'll create a sample DataFrame\n",
        "data = [('James', 34, 55000),\n",
        "('Michael', 30, 70000),\n",
        "('Robert', 37, 60000),\n",
        "('Maria', 29, 80000),\n",
        "('Jen', 32, 65000)]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\" , \"salary\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oL7T0SC9B66",
        "outputId": "b32a067f-a658-4228-c448-345f4ea3f18b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+\n",
            "|   name|age|salary|\n",
            "+-------+---+------+\n",
            "|  James| 34| 55000|\n",
            "|Michael| 30| 70000|\n",
            "| Robert| 37| 60000|\n",
            "|  Maria| 29| 80000|\n",
            "|    Jen| 32| 65000|\n",
            "+-------+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "df = df.withColumn(\"id\", F.monotonically_increasing_id())\n",
        "window = Window.orderBy(\"id\")\n",
        "df = df.withColumn(\"data\", F.lag(F.col(\"salary\"), 1, 0).over(window))\n",
        "df = df.withColumn(\"data\", F.col(\"salary\") - F.lag(F.col(\"salary\"), 1, 0).over(window))\n",
        "df= df.drop(\"id\")\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMENQXGo9JYc",
        "outputId": "51c380c2-29f2-48a7-cb0e-d245301103a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+------+----------+------+\n",
            "|   name|age|salary|  data|prev_value|  diff|\n",
            "+-------+---+------+------+----------+------+\n",
            "|  James| 34| 55000| 55000|      null|     0|\n",
            "|Michael| 30| 70000| 15000|     55000| 15000|\n",
            "| Robert| 37| 60000|-10000|     70000|-10000|\n",
            "|  Maria| 29| 80000| 20000|     60000| 20000|\n",
            "|    Jen| 32| 65000|-15000|     80000|-15000|\n",
            "+-------+---+------+------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Define window specification\n",
        "df = df.withColumn(\"id\", F.monotonically_increasing_id())\n",
        "window = Window.orderBy(\"id\")\n",
        "\n",
        "# Generate the lag of the variable\n",
        "df = df.withColumn(\"prev_value\", F.lag(df.salary).over(window))\n",
        "\n",
        "# Compute the difference with lag\n",
        "df = df.withColumn(\"diff\", F.when(F.isnull(df.salary - df.prev_value), 0)\n",
        ".otherwise(df.salary - df.prev_value)).drop(\"id\")\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMc9py049IU0",
        "outputId": "a7755638-3581-420b-df95-fb30f6553ea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+------+----------+------+\n",
            "|   name|age|salary|  data|prev_value|  diff|\n",
            "+-------+---+------+------+----------+------+\n",
            "|  James| 34| 55000| 55000|      null|     0|\n",
            "|Michael| 30| 70000| 15000|     55000| 15000|\n",
            "| Robert| 37| 60000|-10000|     70000|-10000|\n",
            "|  Maria| 29| 80000| 20000|     60000| 20000|\n",
            "|    Jen| 32| 65000|-15000|     80000|-15000|\n",
            "+-------+---+------+------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **21. How to get the day of month, week number, day of year and day of week from a date strings?**"
      ],
      "metadata": {
        "id": "O5oqj0SR_swL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example data\n",
        "data = [(\"2023-05-18\",\"01 Jan 2010\",), (\"2023-12-31\", \"01 Jan 2010\",)]\n",
        "df = spark.createDataFrame(data, [\"date_str_1\", \"date_str_2\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUcfcdPv_qxo",
        "outputId": "01cd73de-9f45-47d3-8a77-d005bf3c7da9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "|date_str_1| date_str_2|\n",
            "+----------+-----------+\n",
            "|2023-05-18|01 Jan 2010|\n",
            "|2023-12-31|01 Jan 2010|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date, dayofmonth, weekofyear, dayofyear, dayofweek\n",
        "\n",
        "# Assuming df is your DataFrame with the date columns\n",
        "df_updated = df.withColumn(\"dayofmonth_1\", dayofmonth(to_date(df.date_str_1))) \\\n",
        "               .withColumn(\"weekofyear_1\", weekofyear(to_date(df.date_str_1))) \\\n",
        "               .withColumn(\"dayofyear_1\", dayofyear(to_date(df.date_str_1))) \\\n",
        "               .withColumn(\"dayofweek_1\", dayofweek(to_date(df.date_str_1))) \\\n",
        "               .withColumn(\"dayofmonth_2\", dayofmonth(to_date(df.date_str_2, 'dd MMM yyyy'))) \\\n",
        "               .withColumn(\"weekofyear_2\", weekofyear(to_date(df.date_str_2, 'dd MMM yyyy'))) \\\n",
        "               .withColumn(\"dayofyear_2\", dayofyear(to_date(df.date_str_2, 'dd MMM yyyy'))) \\\n",
        "               .withColumn(\"dayofweek_2\", dayofweek(to_date(df.date_str_2, 'dd MMM yyyy')))\n",
        "df_updated.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXd0aPHUWcoC",
        "outputId": "d4dc33e3-4eb9-4342-e647-32724b9b6a9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+------------+------------+-----------+-----------+------------+------------+-----------+-----------+\n",
            "|date_str_1| date_str_2|dayofmonth_1|weekofyear_1|dayofyear_1|dayofweek_1|dayofmonth_2|weekofyear_2|dayofyear_2|dayofweek_2|\n",
            "+----------+-----------+------------+------------+-----------+-----------+------------+------------+-----------+-----------+\n",
            "|2023-05-18|01 Jan 2010|          18|          20|        138|          5|           1|          53|          1|          6|\n",
            "|2023-12-31|01 Jan 2010|          31|          52|        365|          1|           1|          53|          1|          6|\n",
            "+----------+-----------+------------+------------+-----------+-----------+------------+------------+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **22. How to convert year-month string to dates corresponding to the 4th day of the month?**"
      ],
      "metadata": {
        "id": "BOodVH80ZPDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example dataframe\n",
        "df = spark.createDataFrame([('Jan 2010',), ('Feb 2011',), ('Mar 2012',)], ['MonthYear'])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-aZq723ZMZF",
        "outputId": "2387f4a5-4349-46bf-c9a2-cc59dd7f2586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|MonthYear|\n",
            "+---------+\n",
            "| Jan 2010|\n",
            "| Feb 2011|\n",
            "| Mar 2012|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import expr, col\n",
        "\n",
        "# convert YearMonth to date (default to first day of the month)\n",
        "df = df.withColumn('Date', expr(\"to_date(MonthYear, 'MMM yyyy')\"))\n",
        "\n",
        "df.show()\n",
        "\n",
        "df = df.withColumn('Date', expr(\"date_add(date_sub(Date, day(Date) - 1), 3)\"))\n",
        "\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5FXZv4aZa07",
        "outputId": "abc3f40f-13ce-4ba1-c27c-a03f2c917b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+\n",
            "|MonthYear|      Date|\n",
            "+---------+----------+\n",
            "| Jan 2010|2010-01-01|\n",
            "| Feb 2011|2011-02-01|\n",
            "| Mar 2012|2012-03-01|\n",
            "+---------+----------+\n",
            "\n",
            "+---------+----------+\n",
            "|MonthYear|      Date|\n",
            "+---------+----------+\n",
            "| Jan 2010|2010-01-04|\n",
            "| Feb 2011|2011-02-04|\n",
            "| Mar 2012|2012-03-04|\n",
            "+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **23 How to filter words that contain atleast 2 vowels from a series?**"
      ],
      "metadata": {
        "id": "DOuioBnRHohM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame([('Apple',), ('Orange',), ('Plan',) , ('Python',) , ('Money',)], ['Word'])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-6O60EvHoDf",
        "outputId": "43749a92-e768-487f-9226-4b99c69e6051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|  Word|\n",
            "+------+\n",
            "| Apple|\n",
            "|Orange|\n",
            "|  Plan|\n",
            "|Python|\n",
            "| Money|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, length, translate\n",
        "\n",
        "# Filter words that contain at least 2 vowels\n",
        "df_filtered = df.where((length(col('Word')) - length(translate(col('Word'), 'AEIOUaeiou', ''))) >= 2)\n",
        "df_filtered.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxxqQh5aHxe_",
        "outputId": "94f13e40-654a-4291-e6d5-c6162b771b9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|  Word|\n",
            "+------+\n",
            "| Apple|\n",
            "|Orange|\n",
            "| Money|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **24. How to filter valid emails from a list?**"
      ],
      "metadata": {
        "id": "p47DbY5HLx3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list\n",
        "data = ['buying books at amazom.com', 'rameses@egypt.com', 'matt@t.co', 'narendra@modi.com']\n",
        "\n",
        "# Convert the list to DataFrame\n",
        "df = spark.createDataFrame(data, \"string\")\n",
        "df.show(truncate =False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaPDZMb4Lxjw",
        "outputId": "3f3129fe-c215-470f-bcd9-92d26dcaed19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------+\n",
            "|value                     |\n",
            "+--------------------------+\n",
            "|buying books at amazom.com|\n",
            "|rameses@egypt.com         |\n",
            "|matt@t.co                 |\n",
            "|narendra@modi.com         |\n",
            "+--------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "pattern =\"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$\"\n",
        "df_filtered = df.filter(F.col('value').rlike(pattern))\n",
        "df_filtered.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tedy9HIHMq1O",
        "outputId": "f6971dc1-76fd-4458-ae6b-30e6a913e135"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+\n",
            "|            value|\n",
            "+-----------------+\n",
            "|rameses@egypt.com|\n",
            "|        matt@t.co|\n",
            "|narendra@modi.com|\n",
            "+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **25. How to Pivot PySpark DataFrame?**\n"
      ],
      "metadata": {
        "id": "k7lu0Oa9OHzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "data = [\n",
        "(2021, 1, \"US\", 5000),\n",
        "(2021, 1, \"EU\", 4000),\n",
        "(2021, 2, \"US\", 5500),\n",
        "(2021, 2, \"EU\", 4500),\n",
        "(2021, 3, \"US\", 6000),\n",
        "(2021, 3, \"EU\", 5000),\n",
        "(2021, 4, \"US\", 7000),\n",
        "(2021, 4, \"EU\", 6000),\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "columns = [\"year\", \"quarter\", \"region\", \"revenue\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWUlumhDOFmf",
        "outputId": "2cb756c8-efc4-4d6e-fbd8-58a1235491ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+------+-------+\n",
            "|year|quarter|region|revenue|\n",
            "+----+-------+------+-------+\n",
            "|2021|      1|    US|   5000|\n",
            "|2021|      1|    EU|   4000|\n",
            "|2021|      2|    US|   5500|\n",
            "|2021|      2|    EU|   4500|\n",
            "|2021|      3|    US|   6000|\n",
            "|2021|      3|    EU|   5000|\n",
            "|2021|      4|    US|   7000|\n",
            "|2021|      4|    EU|   6000|\n",
            "+----+-------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "pivot_df = df.groupBy(\"year\", \"quarter\").pivot(\"region\").sum(\"revenue\")\n",
        "pivot_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DG-uPo6jObl8",
        "outputId": "f44f40be-3167-4fae-fec7-762531c66c02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+----+----+\n",
            "|year|quarter|  EU|  US|\n",
            "+----+-------+----+----+\n",
            "|2021|      2|4500|5500|\n",
            "|2021|      1|4000|5000|\n",
            "|2021|      4|6000|7000|\n",
            "|2021|      3|5000|6000|\n",
            "+----+-------+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **26. How to get the mean of a variable grouped by another variable?**"
      ],
      "metadata": {
        "id": "fYJvAjzWLwL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "data = [(\"1001\", \"Laptop\", 1000),\n",
        "(\"1002\", \"Mouse\", 50),\n",
        "(\"1003\", \"Laptop\", 1200),\n",
        "(\"1004\", \"Mouse\", 30),\n",
        "(\"1005\", \"Smartphone\", 700)]\n",
        "\n",
        "# Create DataFrame\n",
        "columns = [\"OrderID\", \"Product\", \"Price\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnchGIj0Lr-n",
        "outputId": "94ed7f93-8021-4d95-8bf0-9856c8fffd83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-----+\n",
            "|OrderID|   Product|Price|\n",
            "+-------+----------+-----+\n",
            "|   1001|    Laptop| 1000|\n",
            "|   1002|     Mouse|   50|\n",
            "|   1003|    Laptop| 1200|\n",
            "|   1004|     Mouse|   30|\n",
            "|   1005|Smartphone|  700|\n",
            "+-------+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import mean\n",
        "\n",
        "df1 = df.groupBy(\"Product\").agg(F.avg(\"Price\"))\n",
        "df1.show()\n",
        "\n",
        "result = df.groupBy(\"Product\").agg(mean(\"Price\").alias(\"Total_Sales\"))\n",
        "# Show results\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dSpO5d7MsRB",
        "outputId": "8c1d94ba-2cae-499e-8754-a187f4547609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|   Product|avg(Price)|\n",
            "+----------+----------+\n",
            "|    Laptop|    1100.0|\n",
            "|     Mouse|      40.0|\n",
            "|Smartphone|     700.0|\n",
            "+----------+----------+\n",
            "\n",
            "+----------+-----------+\n",
            "|   Product|Total_Sales|\n",
            "+----------+-----------+\n",
            "|    Laptop|     1100.0|\n",
            "|     Mouse|       40.0|\n",
            "|Smartphone|      700.0|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**27. How to compute the euclidean distance between two columns?**\n"
      ],
      "metadata": {
        "id": "NbxMB8sWPMo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your series\n",
        "data = [(1, 10), (2, 9), (3, 8), (4, 7), (5, 6), (6, 5), (7, 4), (8, 3), (9, 2), (10, 1)]\n",
        "\n",
        "# Convert list to DataFrame\n",
        "df = spark.createDataFrame(data, [\"series1\", \"series2\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxBqgalYO2JF",
        "outputId": "6b236993-106a-4694-d577-77675ea61547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+\n",
            "|series1|series2|\n",
            "+-------+-------+\n",
            "|      1|     10|\n",
            "|      2|      9|\n",
            "|      3|      8|\n",
            "|      4|      7|\n",
            "|      5|      6|\n",
            "|      6|      5|\n",
            "|      7|      4|\n",
            "|      8|      3|\n",
            "|      9|      2|\n",
            "|     10|      1|\n",
            "+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import expr\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Convert series to vectors\n",
        "vecAssembler = VectorAssembler(inputCols=[\"series1\", \"series2\"], outputCol=\"vectors\")\n",
        "df = vecAssembler.transform(df)\n",
        "\n",
        "# Calculate squared differences\n",
        "df = df.withColumn(\"squared_diff\", expr(\"POW(series1 - series2, 2)\"))\n",
        "\n",
        "# Sum squared differences and take square root\n",
        "df.agg(expr(\"SQRT(SUM(squared_diff))\").alias(\"euclidean_distance\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KyxMb8JPdkX",
        "outputId": "228f93cb-4a4e-48d8-ee78-7129b1674738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+\n",
            "|euclidean_distance|\n",
            "+------------------+\n",
            "| 18.16590212458495|\n",
            "+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**28. How to replace missing spaces in a string with the least frequent character?**\n"
      ],
      "metadata": {
        "id": "xl5bfWR9QVKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame([('dbc deb abed gade',),], [\"string\"])\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7V0-tB26QLjo",
        "outputId": "4998bbe0-8b82-4d60-d70c-ee90cae0755d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+\n",
            "|           string|\n",
            "+-----------------+\n",
            "|dbc deb abed gade|\n",
            "+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf, explode\n",
        "from pyspark.sql.types import StringType, ArrayType\n",
        "from collections import Counter\n",
        "\n",
        "def least_freq_char_replace_spaces(s):\n",
        "  counter = Counter(s.replace(\" \", \"\"))\n",
        "  print(counter)\n",
        "  least_freq_char = min(counter, key = counter.get)\n",
        "  print(least_freq_char)\n",
        "\n",
        "  return s.replace(' ', least_freq_char)\n",
        "\n",
        "udf_least_freq_char_replace_spaces = udf(least_freq_char_replace_spaces, StringType())\n",
        "\n",
        "df = spark.createDataFrame([('dbc deb abed gade',)], [\"string\"])\n",
        "df.withColumn('modified_string', udf_least_freq_char_replace_spaces(df['string'])).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjth-7r7QlWZ",
        "outputId": "3f10a103-3a03-460b-fac7-d12797912312"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----------------+\n",
            "|           string|  modified_string|\n",
            "+-----------------+-----------------+\n",
            "|dbc deb abed gade|dbccdebcabedcgade|\n",
            "+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**29. How to create a TimeSeries starting ‘2000-01-01’ and 10 weekends (saturdays) after that having random numbers as values?**\n"
      ],
      "metadata": {
        "id": "Me4oLS2Tiwm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import expr, explode, sequence, rand\n",
        "\n",
        "# Start date and end date (start + 10 weekends)\n",
        "start_date = '2000-01-01'\n",
        "end_date = '2000-03-04' # Calculated manually: 10 weekends (Saturdays) from start date\n",
        "\n",
        "# Create a DataFrame with one row containing a sequence from start_date to end_date with a 1 day step\n",
        "df = spark.range(1).select(\n",
        "explode(\n",
        "sequence(\n",
        "expr(f\"date '{start_date}'\"),\n",
        "expr(f\"date '{end_date}'\"),\n",
        "expr(\"interval 1 day\")\n",
        ")\n",
        ").alias(\"date\")\n",
        ")\n",
        "\n",
        "# Filter out the weekdays (retain weekends)\n",
        "df = df.filter(expr(\"dayofweek(date) = 7\")) # 7 corresponds to Saturday in Spark\n",
        "\n",
        "# Add the random numbers column\n",
        "#df = df.withColumn(\"random_numbers\", rand()*10)\n",
        "df = df.withColumn(\"random_numbers\", ((rand(seed=42) * 10) + 1).cast(\"int\"))\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUenUpuBgp28",
        "outputId": "d30c75c8-a322-41a1-f822-396d06059c18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------+\n",
            "|      date|random_numbers|\n",
            "+----------+--------------+\n",
            "|2000-01-01|             9|\n",
            "|2000-01-08|             7|\n",
            "|2000-01-15|             3|\n",
            "|2000-01-22|             3|\n",
            "|2000-01-29|             7|\n",
            "|2000-02-05|             9|\n",
            "|2000-02-12|             9|\n",
            "|2000-02-19|             8|\n",
            "|2000-02-26|             4|\n",
            "|2000-03-04|             1|\n",
            "+----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**30. How to get the nrows, ncolumns, datatype of a dataframe?**"
      ],
      "metadata": {
        "id": "nPkZo7XjncW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkFiles\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/selva86/datasets/master/Churn_Modelling.csv\"\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "df = spark.read.csv(SparkFiles.get(\"Churn_Modelling.csv\"), header=True, inferSchema=True)\n",
        "\n",
        "#df = spark.read.csv(\"C:/Users/RajeshVaddi/Documents/MLPlus/DataSets/Churn_Modelling.csv\", header=True, inferSchema=True)\n",
        "\n",
        "df.show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjbw1Vi4nieq",
        "outputId": "d0e92fce-d257-48c5-cf01-ae92f3ae0ef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+--------+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+\n",
            "|RowNumber|CustomerId|Surname |CreditScore|Geography|Gender|Age|Tenure|Balance  |NumOfProducts|HasCrCard|IsActiveMember|EstimatedSalary|Exited|\n",
            "+---------+----------+--------+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+\n",
            "|1        |15634602  |Hargrave|619        |France   |Female|42 |2     |0.0      |1            |1        |1             |101348.88      |1     |\n",
            "|2        |15647311  |Hill    |608        |Spain    |Female|41 |1     |83807.86 |1            |0        |1             |112542.58      |0     |\n",
            "|3        |15619304  |Onio    |502        |France   |Female|42 |8     |159660.8 |3            |1        |0             |113931.57      |1     |\n",
            "|4        |15701354  |Boni    |699        |France   |Female|39 |1     |0.0      |2            |0        |0             |93826.63       |0     |\n",
            "|5        |15737888  |Mitchell|850        |Spain    |Female|43 |2     |125510.82|1            |1        |1             |79084.1        |0     |\n",
            "+---------+----------+--------+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nrows = df.count()\n",
        "print(f\"Number of rows: {nrows}\")\n",
        "\n",
        "ncolumns = len(df.columns)\n",
        "print(f\"Number of columns: {ncolumns}\")\n",
        "\n",
        "datatype = df.dtypes\n",
        "print(f\"Data types: {datatype}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIfGeFgPob-7",
        "outputId": "561c1fbc-e0be-46b4-d5e4-bb6da07dfdaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows: 10000\n",
            "Number of columns: 14\n",
            "Data types: [('RowNumber', 'int'), ('CustomerId', 'int'), ('Surname', 'string'), ('CreditScore', 'int'), ('Geography', 'string'), ('Gender', 'string'), ('Age', 'int'), ('Tenure', 'int'), ('Balance', 'double'), ('NumOfProducts', 'int'), ('HasCrCard', 'int'), ('IsActiveMember', 'int'), ('EstimatedSalary', 'double'), ('Exited', 'int')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**31. How to rename a specific columns in a dataframe?**\n",
        "\n"
      ],
      "metadata": {
        "id": "K-epdwK3pUt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppose you have the following DataFrame\n",
        "df = spark.createDataFrame([('Alice', 1, 30),('Bob', 2, 35)], [\"name\", \"age\", \"qty\"])\n",
        "\n",
        "df.show()\n",
        "\n",
        "# Rename lists for specific columns\n",
        "old_names = [\"qty\", \"age\"]\n",
        "new_names = [\"user_qty\", \"user_age\"]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaGFFxazpUSz",
        "outputId": "2034fe2b-5d84-4ce1-8eb8-223e442a9577"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+---+\n",
            "| name|age|qty|\n",
            "+-----+---+---+\n",
            "|Alice|  1| 30|\n",
            "|  Bob|  2| 35|\n",
            "+-----+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df =df.withColumnRenamed(\"qty\", \"quantity\")\n",
        "for old_names,new_names in zip(old_names, new_names):\n",
        "  df = df.withColumnRenamed(old_names, new_names)\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGL6Q6_DpkfI",
        "outputId": "a64cc56d-0936-4792-d381-436e83942fc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------+--------+\n",
            "| name|user_age|quantity|\n",
            "+-----+--------+--------+\n",
            "|Alice|       1|      30|\n",
            "|  Bob|       2|      35|\n",
            "+-----+--------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **32. How to check if a dataframe has any missing values and count of missing values in each column?**"
      ],
      "metadata": {
        "id": "ah83SOnurg6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df is your DataFrame\n",
        "df = spark.createDataFrame([\n",
        "(\"A\", 1, None),\n",
        "(\"B\", None, \"123\" ),\n",
        "(\"B\", 3, \"456\"),\n",
        "(\"D\", None, None),\n",
        "], [\"Name\", \"Value\", \"id\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxfODyjuqMxR",
        "outputId": "66a516cd-a5f0-4bfc-9fb1-6df358fbccc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+----+\n",
            "|Name|Value|  id|\n",
            "+----+-----+----+\n",
            "|   A|    1|null|\n",
            "|   B| null| 123|\n",
            "|   B|    3| 456|\n",
            "|   D| null|null|\n",
            "+----+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "missing = df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns))\n",
        "has_missing = any(row.asDict().values() for row in missing.collect())\n",
        "print(has_missing)\n",
        "\n",
        "missing_count = missing.collect()[0].asDict()\n",
        "print(missing_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emGGaQimrn9I",
        "outputId": "2846eeec-7ccf-4158-d467-f8e174162f29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "{'Name': 0, 'Value': 2, 'id': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **33 How to replace missing values of multiple numeric columns with the mean?**"
      ],
      "metadata": {
        "id": "8Pchm97Ms-v_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame([\n",
        "(\"A\", 1, None),\n",
        "(\"B\", None, 123 ),\n",
        "(\"B\", 3, 456),\n",
        "(\"D\", 6, None),\n",
        "], [\"Name\", \"var1\", \"var2\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syqMGdd_s7OF",
        "outputId": "a031dd9e-43ac-4385-d5aa-c6bce1c5f58a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+\n",
            "|Name|var1|var2|\n",
            "+----+----+----+\n",
            "|   A|   1|null|\n",
            "|   B|null| 123|\n",
            "|   B|   3| 456|\n",
            "|   D|   6|null|\n",
            "+----+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Imputer\n",
        "\n",
        "column_names = [\"var1\", \"var2\"]\n",
        "\n",
        "# Initialize the Imputer\n",
        "imputer = Imputer(inputCols= column_names, outputCols= column_names, strategy=\"mean\")\n",
        "\n",
        "# Fit the Imputer\n",
        "model = imputer.fit(df)\n",
        "\n",
        "#Transform the dataset\n",
        "imputed_df = model.transform(df)\n",
        "\n",
        "imputed_df.show(5)\n",
        "\n",
        "\"\"\"\n",
        "Imputer\n",
        "class pyspark.ml.feature.Imputer(*, strategy: str = 'mean', missingValue: float = nan, inputCols: Optional[List[str]] = None, outputCols: Optional[List[str]] = None, inputCol: Optional[str] = None, outputCol: Optional[str] = None, relativeError: float = 0.001)[source]\n",
        "Imputation estimator for completing missing values, using the mean, median or mode of the columns in which the missing values are located. The input columns should be of numeric type. Currently Imputer does not support categorical features and possibly creates incorrect values for a categorical feature.\n",
        "\n",
        "Note that the mean/median/mode value is computed after filtering out missing values. All Null values in the input columns are treated as missing, and so are also imputed. For computing median,\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrKtKBcytGSX",
        "outputId": "5522db1e-59f1-4cf0-f298-4f4fe819e438"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+\n",
            "|Name|var1|var2|\n",
            "+----+----+----+\n",
            "|   A|   1| 289|\n",
            "|   B|   3| 123|\n",
            "|   B|   3| 456|\n",
            "|   D|   6| 289|\n",
            "+----+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **34. How to change the order of columns of a dataframe?**"
      ],
      "metadata": {
        "id": "KqUCLOlduqG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "data = [(\"John\", \"Doe\", 30), (\"Jane\", \"Doe\", 25), (\"Alice\", \"Smith\", 22)]\n",
        "\n",
        "# Create DataFrame from the data\n",
        "df = spark.createDataFrame(data, [\"First_Name\", \"Last_Name\", \"Age\"])\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABEe55EOupt5",
        "outputId": "cb7e1160-2f26-4776-fc60-a0a24644fd9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+---+\n",
            "|First_Name|Last_Name|Age|\n",
            "+----------+---------+---+\n",
            "|      John|      Doe| 30|\n",
            "|      Jane|      Doe| 25|\n",
            "|     Alice|    Smith| 22|\n",
            "+----------+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_order = [\"Age\", \"First_Name\", \"Last_Name\"]\n",
        "df = df.select(*new_order)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc5FsWtjwgT5",
        "outputId": "7caf403c-1507-4611-a33e-b95e1a6fde17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+---------+\n",
            "|Age|First_Name|Last_Name|\n",
            "+---+----------+---------+\n",
            "| 30|      John|      Doe|\n",
            "| 25|      Jane|      Doe|\n",
            "| 22|     Alice|    Smith|\n",
            "+---+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **35. How to format or suppress scientific notations in a PySpark DataFrame?**"
      ],
      "metadata": {
        "id": "BICL0XDqFjVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have a DataFrame df and the column you want to format is 'your_column'\n",
        "df = spark.createDataFrame([(1, 0.000000123), (2, 0.000023456), (3, 0.000345678)], [\"id\", \"your_column\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WY_Km8srFjEy",
        "outputId": "eaa82886-169c-493a-b42f-9ddb0bc50b2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------+\n",
            "| id|your_column|\n",
            "+---+-----------+\n",
            "|  1|    1.23E-7|\n",
            "|  2|  2.3456E-5|\n",
            "|  3| 3.45678E-4|\n",
            "+---+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import DecimalType\n",
        "from pyspark.sql.functions import format_number\n",
        "\n",
        "decimal_places = 10\n",
        "\n",
        "df = df.withColumn(\"your_column\", format_number(\"your_column\", decimal_places))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgJqgyoQF5J_",
        "outputId": "51939668-dcf8-402e-f889-bdf651e38df4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------+\n",
            "| id| your_column|\n",
            "+---+------------+\n",
            "|  1|0.0000000000|\n",
            "|  2|0.0000000000|\n",
            "|  3|0.0000000000|\n",
            "+---+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **36. How to format all the values in a dataframe as percentages?**\n",
        "\n"
      ],
      "metadata": {
        "id": "uwnokOzaH65U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "data = [(0.1, .08), (0.2, .06), (0.33, .02)]\n",
        "df = spark.createDataFrame(data, [\"numbers_1\", \"numbers_2\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItJvP-chHQzp",
        "outputId": "7b72b91d-a3f9-4dc2-e987-e356aa7fb70d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+\n",
            "|numbers_1|numbers_2|\n",
            "+---------+---------+\n",
            "|      0.1|     0.08|\n",
            "|      0.2|     0.06|\n",
            "|     0.33|     0.02|\n",
            "+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import concat, col, lit\n",
        "\n",
        "#create an empty list to add columns name\n",
        "columns = []\n",
        "\n",
        "#Loop to add columns name from the dataframes into a list\n",
        "for field in df.schema.fields:\n",
        "    columns.append(field.name)\n",
        "\n",
        "#print(columns) --> testin result : is ok\n",
        "\n",
        "#function for percentage into column dataframe pyspark\n",
        "for col_name in columns:\n",
        "  df = df.withColumn(col_name, concat((col(col_name) * 100).cast('decimal(10, 2)'), lit(\"%\")))\n",
        "\n",
        "df.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEhawSNHJWCJ",
        "outputId": "c81f5767-463a-47a8-c2a4-3f84ba99d1f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+\n",
            "|numbers_1|numbers_2|\n",
            "+---------+---------+\n",
            "|   10.00%|    8.00%|\n",
            "|   20.00%|    6.00%|\n",
            "|   33.00%|    2.00%|\n",
            "+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import concat, col, lit\n",
        "\n",
        "columns = [\"numbers_1\", \"numbers_2\"]\n",
        "\n",
        "for col_name in columns:\n",
        "  df = df.withColumn(col_name, concat((col(col_name) * 100).cast('decimal(10, 2)'), lit(\"%\")))\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CscpnkYgIia1",
        "outputId": "3efed6f3-9409-4ea1-e927-844cbf1a0fca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+\n",
            "|numbers_1|numbers_2|\n",
            "+---------+---------+\n",
            "|   10.00%|    8.00%|\n",
            "|   20.00%|    6.00%|\n",
            "|   33.00%|    2.00%|\n",
            "+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **37. How to filter every nth row in a dataframe?**\n"
      ],
      "metadata": {
        "id": "V6MjY3RbN0Kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "data = [(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3), (\"Dave\", 4), (\"Eve\", 5),\n",
        "(\"Frank\", 6), (\"Grace\", 7), (\"Hannah\", 8), (\"Igor\", 9), (\"Jack\", 10)]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Number\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5O09M-5Nuq1",
        "outputId": "0032ec70-f3bd-4662-8a3e-0498bda638de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+\n",
            "|   Name|Number|\n",
            "+-------+------+\n",
            "|  Alice|     1|\n",
            "|    Bob|     2|\n",
            "|Charlie|     3|\n",
            "|   Dave|     4|\n",
            "|    Eve|     5|\n",
            "|  Frank|     6|\n",
            "|  Grace|     7|\n",
            "| Hannah|     8|\n",
            "|   Igor|     9|\n",
            "|   Jack|    10|\n",
            "+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
        "\n",
        "# Define window\n",
        "window = Window.orderBy(monotonically_increasing_id())\n",
        "\n",
        "# Add row_number to DataFrame\n",
        "df = df.withColumn(\"rn\", row_number().over(window))\n",
        "\n",
        "n = 5 # filter every 5th row\n",
        "\n",
        "# Filter every nth row\n",
        "df = df.filter((df.rn % n) == 0)\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtgfXZjDOXuQ",
        "outputId": "5a859dec-b899-4aaa-b017-385ae56cf125"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+---+\n",
            "|Name|Number| rn|\n",
            "+----+------+---+\n",
            "| Eve|     5|  5|\n",
            "|Jack|    10| 10|\n",
            "+----+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **38. How to get the row number of the nth largest value in a column?**"
      ],
      "metadata": {
        "id": "Y7TCjboun7l8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "# Sample Data\n",
        "data = [\n",
        "Row(id=1, column1=5),\n",
        "Row(id=2, column1=8),\n",
        "Row(id=3, column1=12),\n",
        "Row(id=4, column1=1),\n",
        "Row(id=5, column1=15),\n",
        "Row(id=6, column1=7),\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiTlyUjbn7Am",
        "outputId": "6d6d4b41-9c64-4fe4-bfa1-520713d4a9e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|column1|\n",
            "+---+-------+\n",
            "|  1|      5|\n",
            "|  2|      8|\n",
            "|  3|     12|\n",
            "|  4|      1|\n",
            "|  5|     15|\n",
            "|  6|      7|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import desc, row_number\n",
        "\n",
        "window = Window.orderBy(desc(\"column1\"))\n",
        "df = df.withColumn(\"row_number\", row_number().over(window))\n",
        "\n",
        "n = 3 # We're interested in the 3rd largest value.\n",
        "row = df.filter(df.row_number == n).first()\n",
        "\n",
        "if row:\n",
        "  print(\"Row number:\", row.row_number)\n",
        "  print(\"Column value:\", row.column1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFfr4kOIo0u0",
        "outputId": "3bfc1c67-f3b8-4f4c-cab2-e4efd8453dbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row number: 3\n",
            "Column value: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **39.How to get the last n rows of a dataframe with row sum > 100?**"
      ],
      "metadata": {
        "id": "0CxSptHGq6Eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "data = [(10, 25, 70),\n",
        "(40, 5, 20),\n",
        "(70, 80, 100),\n",
        "(10, 2, 60),\n",
        "(40, 50, 20)]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, [\"col1\", \"col2\", \"col3\"])\n",
        "\n",
        "# Display original DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uykzLFqnq5ul",
        "outputId": "86ead113-1591-4609-c07d-921558d68b8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+\n",
            "|col1|col2|col3|\n",
            "+----+----+----+\n",
            "|  10|  25|  70|\n",
            "|  40|   5|  20|\n",
            "|  70|  80| 100|\n",
            "|  10|   2|  60|\n",
            "|  40|  50|  20|\n",
            "+----+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from functools import reduce\n",
        "\n",
        "#In Python, the reduce() function is used to apply a function to an iterable and reduce it to a single cumulative value.\n",
        "#This function is part of the functools module, which must be imported before you can use reduce()\n",
        "\n",
        "\n",
        "# Add 'row_sum' column\n",
        "df = df.withColumn('row_sum', reduce(lambda a, b: a+b, [F.col(x) for x in df.columns]))\n",
        "\n",
        "# Display DataFrame with 'row_sum'\n",
        "df.show()\n",
        "\n",
        "# Filter rows where 'row_sum' > 100\n",
        "df = df.filter(F.col('row_sum') > 100)\n",
        "\n",
        "# Display filtered DataFrame\n",
        "df.show()\n",
        "\n",
        "# Add 'id' column\n",
        "df = df.withColumn('id', F.monotonically_increasing_id())\n",
        "\n",
        "# Get the last 2 rows\n",
        "df_last_2 = df.sort(F.desc('id')).limit(2)\n",
        "\n",
        "# Display the last 2 rows\n",
        "df_last_2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uG2n8sAEpJGE",
        "outputId": "140c074f-3a43-44a4-bb4f-830b223d759e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+-------+\n",
            "|col1|col2|col3|row_sum|\n",
            "+----+----+----+-------+\n",
            "|  10|  25|  70|    105|\n",
            "|  40|   5|  20|     65|\n",
            "|  70|  80| 100|    250|\n",
            "|  10|   2|  60|     72|\n",
            "|  40|  50|  20|    110|\n",
            "+----+----+----+-------+\n",
            "\n",
            "+----+----+----+-------+\n",
            "|col1|col2|col3|row_sum|\n",
            "+----+----+----+-------+\n",
            "|  10|  25|  70|    105|\n",
            "|  70|  80| 100|    250|\n",
            "|  40|  50|  20|    110|\n",
            "+----+----+----+-------+\n",
            "\n",
            "+----+----+----+-------+----------+\n",
            "|col1|col2|col3|row_sum|        id|\n",
            "+----+----+----+-------+----------+\n",
            "|  40|  50|  20|    110|8589934593|\n",
            "|  70|  80| 100|    250|8589934592|\n",
            "+----+----+----+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **40. How to create a column containing the minimum by maximum of each row?**"
      ],
      "metadata": {
        "id": "XFRyV6Bss3mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Data\n",
        "data = [(1, 2, 3), (4, 5, 6), (7, 8, 9), (10, 11, 12)]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, [\"col1\", \"col2\", \"col3\"])\n",
        "\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5Jo3rmdtA-0",
        "outputId": "b8115da2-3bc8-45d8-ba7c-39fa75163fad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+\n",
            "|col1|col2|col3|\n",
            "+----+----+----+\n",
            "|   1|   2|   3|\n",
            "|   4|   5|   6|\n",
            "|   7|   8|   9|\n",
            "|  10|  11|  12|\n",
            "+----+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf, array\n",
        "from pyspark.sql.types import FloatType\n",
        "\n",
        "# Define UDF\n",
        "def min_max_ratio(row):\n",
        "  return float(min(row)) / max(row)\n",
        "\n",
        "min_max_ratio_udf = udf(min_max_ratio, FloatType())\n",
        "\n",
        "# Apply UDF to create new column\n",
        "df = df.withColumn('min_by_max', min_max_ratio_udf(array(df.columns)))\n",
        "\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7z6NVaxtRTq",
        "outputId": "f3f1635f-a5cf-441b-f6c1-d4376833fb79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+----------+\n",
            "|col1|col2|col3|min_by_max|\n",
            "+----+----+----+----------+\n",
            "|   1|   2|   3|0.33333334|\n",
            "|   4|   5|   6| 0.6666667|\n",
            "|   7|   8|   9| 0.7777778|\n",
            "|  10|  11|  12| 0.8333333|\n",
            "+----+----+----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **41. How to create a column that contains the penultimate value in each row?**"
      ],
      "metadata": {
        "id": "MH_UoSNatkiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(10, 20, 30),\n",
        "(40, 60, 50),\n",
        "(80, 70, 90)]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"Column1\", \"Column2\", \"Column3\"])\n",
        "\n",
        "df.show()\n",
        "\n",
        "#The penultimate value refers to the second-to-last value in a sequence or list. For example,\n",
        "#if you have a list of numbers [10, 20, 30, 40, 50], the penultimate value would be 40. It’s essentially the value right before the last one.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMkrDtaitj9H",
        "outputId": "d55ba1fe-48a6-4e71-b796-4b927cbb8453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-------+\n",
            "|Column1|Column2|Column3|\n",
            "+-------+-------+-------+\n",
            "|     10|     20|     30|\n",
            "|     40|     60|     50|\n",
            "|     80|     70|     90|\n",
            "+-------+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import ArrayType, IntegerType\n",
        "\n",
        "# Define UDF to sort array in descending order\n",
        "sort_array_desc = F.udf(lambda arr: sorted(arr), ArrayType(IntegerType()))\n",
        "\n",
        "# Create array from columns, sort in descending order and get the penultimate value\n",
        "df = df.withColumn(\"row_as_array\", sort_array_desc(F.array(df.columns)))\n",
        "df = df.withColumn(\"Penultimate\", df['row_as_array'].getItem(1))\n",
        "df = df.drop('row_as_array')\n",
        "\n",
        "df.show()\n",
        "\n",
        "\"\"\"\n",
        "UDF’s a.k.a User Defined Functions, If you are coming from SQL background, UDF’s are nothing new to you as most of the traditional RDBMS databases support User Defined Functions,\n",
        "these functions need to register in the database library and use them on SQL as regular functions\n",
        "\n",
        "PySpark UDF’s are similar to UDF on traditional databases. In PySpark,\n",
        "you create a function in a Python syntax and wrap it with PySpark SQL udf() or register it as udf and use it on DataFrame and SQL respectively.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waXyuABftyFp",
        "outputId": "c68dff4d-f139-4c44-b752-9ecc01dddfb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-------+-----------+\n",
            "|Column1|Column2|Column3|Penultimate|\n",
            "+-------+-------+-------+-----------+\n",
            "|     10|     20|     30|         20|\n",
            "|     40|     60|     50|         50|\n",
            "|     80|     70|     90|         80|\n",
            "+-------+-------+-------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **42. How to normalize all columns in a dataframe?**"
      ],
      "metadata": {
        "id": "uIR2SWxnxL77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a sample dataframe\n",
        "data = [(1, 2, 3),\n",
        "(2, 3, 4),\n",
        "(3, 4, 5),\n",
        "(4, 5, 6)]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"Col1\", \"Col2\", \"Col3\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQ6XA5ulzVUC",
        "outputId": "d7e82655-4ec1-4807-d2d3-4a237a72e4e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+\n",
            "|Col1|Col2|Col3|\n",
            "+----+----+----+\n",
            "|   1|   2|   3|\n",
            "|   2|   3|   4|\n",
            "|   3|   4|   5|\n",
            "|   4|   5|   6|\n",
            "+----+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "#define the list of columns to be normalized\n",
        "columns_to_normalize = [\"Col1\", \"Col2\", \"Col3\"]\n",
        "\n",
        "#create a vector assembler to combine the columns into a single vector column\n",
        "assembler = VectorAssembler(inputCols=columns_to_normalize, outputCol=\"features\")\n",
        "# transform the data\n",
        "df_assembled  = assembler.transform(df)\n",
        "\n",
        "#create a standard scaler to normalize the features\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
        "\n",
        "# fit and transform the data\n",
        "scalerModel = scaler.fit(df_assembled)\n",
        "df_normalized = scalerModel.transform(df_assembled)\n",
        "\n",
        "# if you want to drop the original 'features' column\n",
        "df_normalized = df_normalized.drop('features')\n",
        "\n",
        "df_normalized.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsSzmpiIzX4i",
        "outputId": "a4efc90f-7fa4-4735-d1f0-52087f735df0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+-------------------------------------------------------------+\n",
            "|Col1|Col2|Col3|scaled_features                                              |\n",
            "+----+----+----+-------------------------------------------------------------+\n",
            "|1   |2   |3   |[-1.161895003862225,-1.161895003862225,-1.161895003862225]   |\n",
            "|2   |3   |4   |[-0.3872983346207417,-0.3872983346207417,-0.3872983346207417]|\n",
            "|3   |4   |5   |[0.3872983346207417,0.3872983346207417,0.3872983346207417]   |\n",
            "|4   |5   |6   |[1.161895003862225,1.161895003862225,1.161895003862225]      |\n",
            "+----+----+----+-------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **43. How to get the positions where values of two columns match?**"
      ],
      "metadata": {
        "id": "7FYg6z9_23Pa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sample DataFrame\n",
        "data = [(\"John\", \"John\"), (\"Lily\", \"Lucy\"), (\"Sam\", \"Sam\"), (\"Lucy\", \"Lily\")]\n",
        "df = spark.createDataFrame(data, [\"Name1\", \"Name2\"])\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMMNFS4h28nK",
        "outputId": "1ff0fbe6-c410-4fab-fd88-baef740d8887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "|Name1|Name2|\n",
            "+-----+-----+\n",
            "| John| John|\n",
            "| Lily| Lucy|\n",
            "|  Sam|  Sam|\n",
            "| Lucy| Lily|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df = df.withColumn(\"position\", when(col(\"Name1\") == col(\"Name2\"), 1).otherwise(0))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a014_AFz2_25",
        "outputId": "56fcce44-11d4-4c03-b4f1-914141c9443e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+--------+\n",
            "|Name1|Name2|position|\n",
            "+-----+-----+--------+\n",
            "| John| John|       1|\n",
            "| Lily| Lucy|       0|\n",
            "|  Sam|  Sam|       1|\n",
            "| Lucy| Lily|       0|\n",
            "+-----+-----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **44. How to create lags and leads of a column by group in a dataframe?**"
      ],
      "metadata": {
        "id": "14lgUnA43TS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sample DataFrame\n",
        "data = [(\"2023-01-01\", \"Store1\", 100),\n",
        "(\"2023-01-02\", \"Store1\", 150),\n",
        "(\"2023-01-03\", \"Store1\", 200),\n",
        "(\"2023-01-04\", \"Store1\", 250),\n",
        "(\"2023-01-05\", \"Store1\", 300),\n",
        "(\"2023-01-01\", \"Store2\", 50),\n",
        "(\"2023-01-02\", \"Store2\", 60),\n",
        "(\"2023-01-03\", \"Store2\", 80),\n",
        "(\"2023-01-04\", \"Store2\", 90),\n",
        "(\"2023-01-05\", \"Store2\", 120)]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"Date\", \"Store\", \"Sales\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBNH1eRP3TAr",
        "outputId": "d142ca03-3441-4e75-db47-28346e52871d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+-----+\n",
            "|      Date| Store|Sales|\n",
            "+----------+------+-----+\n",
            "|2023-01-01|Store1|  100|\n",
            "|2023-01-02|Store1|  150|\n",
            "|2023-01-03|Store1|  200|\n",
            "|2023-01-04|Store1|  250|\n",
            "|2023-01-05|Store1|  300|\n",
            "|2023-01-01|Store2|   50|\n",
            "|2023-01-02|Store2|   60|\n",
            "|2023-01-03|Store2|   80|\n",
            "|2023-01-04|Store2|   90|\n",
            "|2023-01-05|Store2|  120|\n",
            "+----------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lag, lead, to_date\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Convert the date from string to date type\n",
        "df = df.withColumn(\"Date\", to_date(df.Date, 'yyyy-MM-dd'))\n",
        "\n",
        "# Create a Window partitioned by Store, ordered by Date\n",
        "windowSpec = Window.partitionBy(\"Store\").orderBy(\"Date\")\n",
        "\n",
        "# Create lag and lead variables\n",
        "df = df.withColumn(\"Lag_Sales\", lag(df[\"Sales\"]).over(windowSpec))\n",
        "df = df.withColumn(\"Lead_Sales\", lead(df[\"Sales\"]).over(windowSpec))\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSvtZslL3ZdB",
        "outputId": "cc9c376e-2987-479e-c9a0-fc5220be96f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+-----+---------+----------+\n",
            "|      Date| Store|Sales|Lag_Sales|Lead_Sales|\n",
            "+----------+------+-----+---------+----------+\n",
            "|2023-01-01|Store1|  100|     null|       150|\n",
            "|2023-01-02|Store1|  150|      100|       200|\n",
            "|2023-01-03|Store1|  200|      150|       250|\n",
            "|2023-01-04|Store1|  250|      200|       300|\n",
            "|2023-01-05|Store1|  300|      250|      null|\n",
            "|2023-01-01|Store2|   50|     null|        60|\n",
            "|2023-01-02|Store2|   60|       50|        80|\n",
            "|2023-01-03|Store2|   80|       60|        90|\n",
            "|2023-01-04|Store2|   90|       80|       120|\n",
            "|2023-01-05|Store2|  120|       90|      null|\n",
            "+----------+------+-----+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**45. How to get the frequency of unique values in the entire dataframe?**"
      ],
      "metadata": {
        "id": "ocdo19x33pWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a numeric DataFrame\n",
        "data = [(1, 2, 3),\n",
        "(2, 3, 4),\n",
        "(1, 2, 3),\n",
        "(4, 5, 6),\n",
        "(2, 3, 4)]\n",
        "df = spark.createDataFrame(data, [\"Column1\", \"Column2\", \"Column3\"])\n",
        "\n",
        "# Print DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcQbZmJz3n4C",
        "outputId": "c44a42b0-b354-4c7d-c639-5a1972e172ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-------+\n",
            "|Column1|Column2|Column3|\n",
            "+-------+-------+-------+\n",
            "|      1|      2|      3|\n",
            "|      2|      3|      4|\n",
            "|      1|      2|      3|\n",
            "|      4|      5|      6|\n",
            "|      2|      3|      4|\n",
            "+-------+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# get column names\n",
        "columns = df.columns\n",
        "\n",
        "# stack all columns into a single column\n",
        "df_single = None\n",
        "\n",
        "for c in columns:\n",
        "  if df_single is None:\n",
        "    df_single = df.select(col(c).alias(\"single_column\"))\n",
        "  else:\n",
        "    df_single = df_single.union(df.select(col(c).alias(\"single_column\")))\n",
        "\n",
        "# generate frequency table\n",
        "frequency_table = df_single.groupBy(\"single_column\").count().orderBy('count', ascending=False)\n",
        "\n",
        "# show frequency table\n",
        "frequency_table.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3w3vQJL35ZR",
        "outputId": "a9930250-d838-4aa5-d958-e9992ed7c342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----+\n",
            "|single_column|count|\n",
            "+-------------+-----+\n",
            "|            2|    4|\n",
            "|            3|    4|\n",
            "|            4|    3|\n",
            "|            1|    2|\n",
            "|            5|    1|\n",
            "|            6|    1|\n",
            "+-------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **46. How to replace both the diagonals of dataframe with 0?**"
      ],
      "metadata": {
        "id": "xvr_7O-93-Sa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a numeric DataFrame\n",
        "data = [(1, 2, 3, 4),\n",
        "(2, 3, 4, 5),\n",
        "(1, 2, 3, 4),\n",
        "(4, 5, 6, 7)]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"col_1\", \"col_2\", \"col_3\", \"col_4\"])\n",
        "\n",
        "# Print DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lck0TJca39rG",
        "outputId": "c3f1766c-9564-467b-b349-35934046c8a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----+-----+\n",
            "|col_1|col_2|col_3|col_4|\n",
            "+-----+-----+-----+-----+\n",
            "|    1|    2|    3|    4|\n",
            "|    2|    3|    4|    5|\n",
            "|    1|    2|    3|    4|\n",
            "|    4|    5|    6|    7|\n",
            "+-----+-----+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **47. How to reverse the rows of a dataframe?**"
      ],
      "metadata": {
        "id": "MQ9DfkF3EpPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a numeric DataFrame\n",
        "data = [(1, 2, 3, 4),\n",
        "(2, 3, 4, 5),\n",
        "(3, 4, 5, 6),\n",
        "(4, 5, 6, 7)]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"col_1\", \"col_2\", \"col_3\", \"col_4\"])\n",
        "\n",
        "# Print DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZONj6mu4FBo",
        "outputId": "3fad4d87-7832-436d-ef24-97d77ea83f49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----+-----+\n",
            "|col_1|col_2|col_3|col_4|\n",
            "+-----+-----+-----+-----+\n",
            "|    1|    2|    3|    4|\n",
            "|    2|    3|    4|    5|\n",
            "|    3|    4|    5|    6|\n",
            "|    4|    5|    6|    7|\n",
            "+-----+-----+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
        "w = Window.orderBy(monotonically_increasing_id())\n",
        "df = df.withColumn(\"row_number\", row_number().over(w))\n",
        "df = df.orderBy(df[\"row_number\"].desc())\n",
        "df = df.drop(\"row_number\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lmN3UlWEtsS",
        "outputId": "3b7d39f0-0f83-4938-e594-32ec3f336f22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----+-----+\n",
            "|col_1|col_2|col_3|col_4|\n",
            "+-----+-----+-----+-----+\n",
            "|    4|    5|    6|    7|\n",
            "|    3|    4|    5|    6|\n",
            "|    2|    3|    4|    5|\n",
            "|    1|    2|    3|    4|\n",
            "+-----+-----+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **48. How to create one-hot encodings of a categorical variable (dummy variables)?**"
      ],
      "metadata": {
        "id": "tna_FnRcFXom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"A\", 10),(\"A\", 20),(\"B\", 30),(\"B\", 20),(\"B\", 30),(\"C\", 40),(\"C\", 10),(\"D\", 10)]\n",
        "columns = [\"Categories\", \"Value\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dI7v7EZTFVTu",
        "outputId": "a238d0db-01bb-4197-a205-508a4de9cf26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|Categories|Value|\n",
            "+----------+-----+\n",
            "|         A|   10|\n",
            "|         A|   20|\n",
            "|         B|   30|\n",
            "|         B|   20|\n",
            "|         B|   30|\n",
            "|         C|   40|\n",
            "|         C|   10|\n",
            "|         D|   10|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The OneHotEncoder class in PySpark's ml.feature module **is used to convert categorical data (like \"apple,\" \"banana,\" \"cherry\") into a numerical format that machine learning algorithms can understand**. Specifically,** it transforms each category into a unique binary vector, where:**\n",
        "\n",
        "**Each category is represented by a vector (array) containing mostly zeros, with a single \"1\" in the position corresponding to that category**.\n",
        "For example*, if there are 5 categories, a value of 2.0 could be encoded as [0.0, 0.0, 1.0, 0.0].*\n",
        "Key points:\n",
        "\n",
        "dropLast (default=True): This setting means that the last category is not included in the encoding. This is done to avoid redundancy because all categories add up to one vector. For instance, if there are 4 categories and the last one is not included, an input value of 3.0 might be represented as [0.0, 0.0, 0.0] instead of [0.0, 0.0, 0.0, 1.0].\n",
        "\n",
        "handleInvalid ('error' or 'keep'): This parameter controls what happens when the encoder encounters an invalid category (one that wasn't seen during training):\n",
        "\n",
        "'error': Throws an error for invalid categories.\n",
        "'keep': Adds an extra category for invalid values, which, if dropLast is true, will also result in an all-zero vector.\n",
        "Use Case:\n",
        "This encoder is useful in preparing categorical features for algorithms that require numerical input, ensuring that each category is uniquely represented without implying any ordinal relationship between categories."
      ],
      "metadata": {
        "id": "YU6YcWeXHTv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
        "#from pyspark.sql.types import StringType, StructType, StructField\n",
        "\n",
        "# StringIndexer Initialization\n",
        "indexer = StringIndexer(inputCol=\"Categories\", outputCol=\"Categories_Indexed\")\n",
        "indexerModel = indexer.fit(df)\n",
        "\n",
        "# Transform the DataFrame using the fitted StringIndexer model\n",
        "indexed_df = indexerModel.transform(df)\n",
        "#indexed_df.show()\n",
        "\n",
        "encoder = OneHotEncoder(inputCol=\"Categories_Indexed\", outputCol=\"Categories_onehot\")\n",
        "encoded_df = encoder.fit(indexed_df).transform(indexed_df)\n",
        "encoded_df = encoded_df.drop(\"Categories_Indexed\")\n",
        "encoded_df.show(truncate=False)"
      ],
      "metadata": {
        "id": "f5DJxLHKF_hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **49. How to Pivot the dataframe (converting rows into columns) ?**"
      ],
      "metadata": {
        "id": "mTpwrSXvHnuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "data = [\n",
        "(2021, 1, \"US\", 5000),\n",
        "(2021, 1, \"EU\", 4000),\n",
        "(2021, 2, \"US\", 5500),\n",
        "(2021, 2, \"EU\", 4500),\n",
        "(2021, 3, \"US\", 6000),\n",
        "(2021, 3, \"EU\", 5000),\n",
        "(2021, 4, \"US\", 7000),\n",
        "(2021, 4, \"EU\", 6000),\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "columns = [\"year\", \"quarter\", \"region\", \"revenue\"]\n",
        "df = spark.createDataFrame(data, columns)"
      ],
      "metadata": {
        "id": "qZz3QiOlHkxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"year\",\"quarter\")\\\n",
        "  .pivot(\"region\")\\\n",
        "  .agg(F.sum(\"revenue\"))\\\n",
        "  .show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEPek7jPHurC",
        "outputId": "0442d4dc-dc07-4c63-c973-627c8cef1e96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+----+----+\n",
            "|year|quarter|  EU|  US|\n",
            "+----+-------+----+----+\n",
            "|2021|      2|4500|5500|\n",
            "|2021|      1|4000|5000|\n",
            "|2021|      4|6000|7000|\n",
            "|2021|      3|5000|6000|\n",
            "+----+-------+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **50. How to UnPivot the dataframe (converting columns into rows) ?**"
      ],
      "metadata": {
        "id": "JvMcqRoHIhWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "data = [(2021, 2, 4500, 5500),\n",
        "(2021, 1, 4000, 5000),\n",
        "(2021, 3, 5000, 6000),\n",
        "(2021, 4, 6000, 7000)]\n",
        "\n",
        "# Create DataFrame\n",
        "columns = [\"year\", \"quarter\", \"EU\", \"US\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wP3RSb_1JeXw",
        "outputId": "b10fb055-eaaf-489f-e722-2cabb535030c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+----+----+\n",
            "|year|quarter|  EU|  US|\n",
            "+----+-------+----+----+\n",
            "|2021|      2|4500|5500|\n",
            "|2021|      1|4000|5000|\n",
            "|2021|      3|5000|6000|\n",
            "|2021|      4|6000|7000|\n",
            "+----+-------+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unpivot is a reverse operation, we can achieve by rotating column values into rows values. PySpark SQL doesn’t have unpivot function hence will use the stack() function. Below code converts column countries to row."
      ],
      "metadata": {
        "id": "lvgnsIQ7Jvnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import expr\n",
        "\n",
        "unpivotExpr =\"stack(2, 'EU' , EU, 'US',US) as (Region,Revenue)\"\n",
        "unPivotDF = df.select(\"year\",\"quarter\",expr(unpivotExpr)).where(\"Revenue is not null\")\n",
        "unPivotDF.show(truncate=False)\n",
        "unPivotDF.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0VbxYcgJ9r1",
        "outputId": "72b31dab-2b0f-4bed-aa79-1eab9cabba8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+------+-------+\n",
            "|year|quarter|Region|Revenue|\n",
            "+----+-------+------+-------+\n",
            "|2021|2      |EU    |4500   |\n",
            "|2021|2      |US    |5500   |\n",
            "|2021|1      |EU    |4000   |\n",
            "|2021|1      |US    |5000   |\n",
            "|2021|3      |EU    |5000   |\n",
            "|2021|3      |US    |6000   |\n",
            "|2021|4      |EU    |6000   |\n",
            "|2021|4      |US    |7000   |\n",
            "+----+-------+------+-------+\n",
            "\n",
            "+----+-------+------+-------+\n",
            "|year|quarter|Region|Revenue|\n",
            "+----+-------+------+-------+\n",
            "|2021|      2|    EU|   4500|\n",
            "|2021|      2|    US|   5500|\n",
            "|2021|      1|    EU|   4000|\n",
            "|2021|      1|    US|   5000|\n",
            "|2021|      3|    EU|   5000|\n",
            "|2021|      3|    US|   6000|\n",
            "|2021|      4|    EU|   6000|\n",
            "|2021|      4|    US|   7000|\n",
            "+----+-------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **51. How to impute missing values with Zero?**"
      ],
      "metadata": {
        "id": "rv4kMQzzLBLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppose df is your DataFrame\n",
        "df = spark.createDataFrame([(1, None), (None, 2), (3, 4), (5, None)], [\"a\", \"b\"])\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0xwrG1ELDWx",
        "outputId": "9f9905f8-82c7-4123-8d71-c2d81d6418cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+\n",
            "|   a|   b|\n",
            "+----+----+\n",
            "|   1|null|\n",
            "|null|   2|\n",
            "|   3|   4|\n",
            "|   5|null|\n",
            "+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.fillna(0)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNeRtEvxLgh6",
        "outputId": "556f24f3-5338-4adb-9505-ab5315a6614c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+\n",
            "|  a|  b|\n",
            "+---+---+\n",
            "|  1|  0|\n",
            "|  0|  2|\n",
            "|  3|  4|\n",
            "|  5|  0|\n",
            "+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **52. How to identify continuous variables in a dataframe and create a list of those column names?**"
      ],
      "metadata": {
        "id": "YgBmLzLLLuwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkFiles\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/selva86/datasets/master/Churn_Modelling_m.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "df = spark.read.csv(SparkFiles.get(\"Churn_Modelling_m.csv\"), header=True, inferSchema=True)\n",
        "\n",
        "#df = spark.read.csv(\"C:/Users/RajeshVaddi/Documents/MLPlus/DataSets/Churn_Modelling_m.csv\", header=True, inferSchema=True)\n",
        "\n",
        "df.show(2, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6q_erMgLr5Z",
        "outputId": "1f9ea335-2837-4564-c37c-eab8f7a9563b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+--------+-----------+---------+------+---+------+--------+-------------+---------+--------------+---------------+------+\n",
            "|RowNumber|CustomerId|Surname |CreditScore|Geography|Gender|Age|Tenure|Balance |NumOfProducts|HasCrCard|IsActiveMember|EstimatedSalary|Exited|\n",
            "+---------+----------+--------+-----------+---------+------+---+------+--------+-------------+---------+--------------+---------------+------+\n",
            "|1        |15634602  |Hargrave|619        |France   |Female|42 |2     |0.0     |1            |1        |1             |101348.88      |1     |\n",
            "|2        |15647311  |Hill    |608        |Spain    |Female|41 |1     |83807.86|1            |0        |1             |112542.58      |0     |\n",
            "+---------+----------+--------+-----------+---------+------+---+------+--------+-------------+---------+--------------+---------------+------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark Variable type Identification – A Comprehensive Guide to Identifying Discrete, Categorical, and Continuous Variables in Data\n",
        "\n",
        "Let’s Explore what are discrete, categorical, and continuous variables, their identification techniques, and their importance in machine learning and statistical modeling.\n",
        "Data preprocessing is a critical step in machine learning and statistical modeling. Before diving into model building, it is essential to understand and identify the types of variables present in the dataset.\n",
        "\n",
        "Furthermore, I will provide a PySpark function to identify variable types in a PySpark DataFrame.\n",
        "\n",
        "*Types of Variables:*\n",
        "a. **Discrete Variables**: Discrete variables represent countable data, typically integers. Examples include the number of employees in a company or the number of students in a class.\n",
        "\n",
        "b. **Categorical Variables:** Categorical variables represent data that can be divided into distinct categories, such as gender or eye color. They can be either nominal (no order) or ordinal (with a meaningful order).\n",
        "\n",
        "**c. Continuous Variables:** Continuous variables represent data that can take any value within a given range. Examples include height, weight, and temperature.\n",
        "\n",
        "\n",
        "*How to Identifying Variable types?*\n",
        "\n",
        "**a. Discrete Variables: **Check if the variable represents countable data and has a limited number of unique values.\n",
        "\n",
        "**b. Categorical Variables: **Check if the variable can be divided into distinct categories. Ordinal variables can be identified by a meaningful order among categories.\n",
        "\n",
        "**c. Continuous Variables:** Check if the variable can take any value within a given range, and if the data has a continuous distribution."
      ],
      "metadata": {
        "id": "LOsag_5vMXrc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**threshold refers to a specific value or limit that, when exceeded, triggers a certain action, response, or event.** Thresholds are commonly used in various areas of IT, such as monitoring systems, data analysis, cybersecurity, and performance optimization."
      ],
      "metadata": {
        "id": "dLBKSS7SOJIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import IntegerType, StringType, NumericType\n",
        "from pyspark.sql.functions import approxCountDistinct\n",
        "\n",
        "def detect_continuous_variables(df, distinct_threshold):\n",
        "  \"\"\"\n",
        "  Identify continuous variables in a PySpark DataFrame.\n",
        "  :param df: The input PySpark DataFrame\n",
        "  :param distinct_threshold: Threshold to qualify as continuous variables - Count of distinct values > distinct_threshold\n",
        "  :return: A List containing names of continuous variables\n",
        "  \"\"\"\n",
        "  continuous_columns = []\n",
        "  for column in df.columns:\n",
        "    dtype = df.schema[column].dataType\n",
        "    if isinstance(dtype, (IntegerType, NumericType)):\n",
        "      distinct_count = df.select(approxCountDistinct(column)).collect()[0][0]\n",
        "      if distinct_count > distinct_threshold:\n",
        "        continuous_columns.append(column)\n",
        "\n",
        "  return continuous_columns\n",
        "\n",
        "continuous_variables = detect_continuous_variables(df, 10)\n",
        "print(continuous_variables)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe4VQeVaMEZM",
        "outputId": "33d01203-e5b4-412f-e6a9-b4fcd3fc39d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['RowNumber', 'CustomerId', 'CreditScore', 'Age', 'Tenure', 'Balance', 'EstimatedSalary']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **53. How to calculate Mode of a PySpark DataFrame column?**"
      ],
      "metadata": {
        "id": "AohNgXB2PDnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sample DataFrame\n",
        "# https://www.statology.org/pyspark-mode-of-column/\n",
        "# The mode in statistics is the number that appears most often in a given set of data\n",
        "data = [(1, 2, 3), (2, 2, 3), (2, 2, 4), (1, 2, 3), (1, 1, 3)]\n",
        "columns = [\"col1\", \"col2\", \"col3\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkjprYKbOREa",
        "outputId": "230a3497-f5d6-4d56-9e7d-e0935e7ce7b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+\n",
            "|col1|col2|col3|\n",
            "+----+----+----+\n",
            "|   1|   2|   3|\n",
            "|   2|   2|   3|\n",
            "|   2|   2|   4|\n",
            "|   1|   2|   3|\n",
            "|   1|   1|   3|\n",
            "+----+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_grouped = df.groupBy('col2').count()\n",
        "mode_df = df_grouped.orderBy(col('count').desc()).limit(1)\n",
        "\n",
        "mode_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lgRcTiiPHDa",
        "outputId": "8e4e8a63-c792-4d5d-94e1-d9fbc13dcaa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "|col2|count|\n",
            "+----+-----+\n",
            "|   2|    4|\n",
            "+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **54. How to find installed location of Apache Spark and PySpark?**"
      ],
      "metadata": {
        "id": "1aZrQCByRG-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "import os\n",
        "import pyspark\n",
        "#Version 1\n",
        "findspark.init()\n",
        "print(findspark.find())\n",
        "#Version 2\n",
        "print(os.path.dirname(pyspark.__file__))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASRIL4wpRLNn",
        "outputId": "76fa5f6c-208d-4f14-c497-4f80e65f0e51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark\n",
            "/usr/local/lib/python3.10/dist-packages/pyspark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **55. How to convert a column to lower case using UDF?**"
      ],
      "metadata": {
        "id": "PnRzO719RjFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame to test\n",
        "data = [('John Doe', 'NEW YORK'),\n",
        "('Jane Doe', 'LOS ANGELES'),\n",
        "('Mike Johnson', 'CHICAGO'),\n",
        "('Sara Smith', 'SAN FRANCISCO')]\n",
        "\n",
        "df = spark.createDataFrame(data, ['Name', 'City'])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trgm70CPRgdN",
        "outputId": "48d751a8-d8e2-4cb6-ba24-6488e1fc4151"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-------------+\n",
            "|        Name|         City|\n",
            "+------------+-------------+\n",
            "|    John Doe|     NEW YORK|\n",
            "|    Jane Doe|  LOS ANGELES|\n",
            "|Mike Johnson|      CHICAGO|\n",
            "|  Sara Smith|SAN FRANCISCO|\n",
            "+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "UDF’s a.k.a User Defined Functions, If you are coming from SQL background,\n",
        "UDF’s are nothing new to you as most of the traditional RDBMS databases support User Defined Functions,\n",
        "these functions need to register in the database library and use them on SQL as regular functions.\n",
        "\n",
        "PySpark UDF’s are similar to UDF on traditional databases. In PySpark,\n",
        "you create a function in a Python syntax and wrap it with PySpark SQL udf() or register it as udf and use it on DataFrame and SQL respectively.\n",
        "\"\"\"\n",
        "def tolower(s):\n",
        "  if s is not None:\n",
        "    return s.lower()\n",
        "\n",
        "tolower_udf = udf(tolower, StringType())\n",
        "\n",
        "df = df.withColumn('City', tolower_udf(df['City']))\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jq6RbV0ySpp5",
        "outputId": "d0ddd9e7-e69e-4348-8b84-27e6c00671c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-------------+\n",
            "|        Name|         City|\n",
            "+------------+-------------+\n",
            "|    John Doe|     new york|\n",
            "|    Jane Doe|  los angeles|\n",
            "|Mike Johnson|      chicago|\n",
            "|  Sara Smith|san francisco|\n",
            "+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **56. How to convert PySpark data frame to pandas dataframe?**"
      ],
      "metadata": {
        "id": "CjCBcLPjTZPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame to test\n",
        "data = [('John Doe', 'NEW YORK'),\n",
        "('Jane Doe', 'LOS ANGELES'),\n",
        "('Mike Johnson', 'CHICAGO'),\n",
        "('Sara Smith', 'SAN FRANCISCO')]\n",
        "\n",
        "pysparkDF = spark.createDataFrame(data, ['Name', 'City'])\n",
        "\n",
        "pysparkDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9N5id28pTbPG",
        "outputId": "dd6cb8e0-d5ba-4af7-a022-068747930a16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-------------+\n",
            "|        Name|         City|\n",
            "+------------+-------------+\n",
            "|    John Doe|     NEW YORK|\n",
            "|    Jane Doe|  LOS ANGELES|\n",
            "|Mike Johnson|      CHICAGO|\n",
            "|  Sara Smith|SAN FRANCISCO|\n",
            "+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pandas_df = pysparkDF.toPandas()\n",
        "print(pandas_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q56m4OvVTih-",
        "outputId": "d195d30c-59ed-4ce0-d195-55e94bddbe7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Name           City\n",
            "0      John Doe       NEW YORK\n",
            "1      Jane Doe    LOS ANGELES\n",
            "2  Mike Johnson        CHICAGO\n",
            "3    Sara Smith  SAN FRANCISCO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **57. How to View PySpark Cluster Details?**"
      ],
      "metadata": {
        "id": "CrQe3c15TvGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(spark.sparkContext.uiWebUrl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1ksNQeRTxZx",
        "outputId": "51183158-fedc-4a6c-a44c-d0d55e6de4db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "http://d305bb521c3d:4040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **58. How to View PySpark Cluster Configuration Details?**"
      ],
      "metadata": {
        "id": "Bnq7YmxkT4s9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configurations = spark.sparkContext.getConf().getAll()\n",
        "for item in configurations: print(item)\n",
        "\n",
        "# Print all configurations\n",
        "for k,v in spark.sparkContext.getConf().getAll():\n",
        "  print(f\"{k} : {v}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XllxnOOT4RV",
        "outputId": "a05a9a0d-6596-49fa-ff9c-ea2d8188ffd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('spark.app.name', 'PySpark 101 Exercises')\n",
            "('spark.driver.host', '30563843d879')\n",
            "('spark.app.startTime', '1722971858665')\n",
            "('spark.driver.port', '36461')\n",
            "('spark.executor.id', 'driver')\n",
            "('spark.app.id', 'local-1722971864050')\n",
            "('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n",
            "('spark.rdd.compress', 'True')\n",
            "('spark.app.submitTime', '1722971858018')\n",
            "('spark.serializer.objectStreamReset', '100')\n",
            "('spark.master', 'local[*]')\n",
            "('spark.submit.pyFiles', '')\n",
            "('spark.submit.deployMode', 'client')\n",
            "('spark.ui.showConsoleProgress', 'true')\n",
            "('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n",
            "spark.app.name : PySpark 101 Exercises\n",
            "spark.driver.host : 30563843d879\n",
            "spark.app.startTime : 1722971858665\n",
            "spark.driver.port : 36461\n",
            "spark.executor.id : driver\n",
            "spark.app.id : local-1722971864050\n",
            "spark.driver.extraJavaOptions : -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
            "spark.rdd.compress : True\n",
            "spark.app.submitTime : 1722971858018\n",
            "spark.serializer.objectStreamReset : 100\n",
            "spark.master : local[*]\n",
            "spark.submit.pyFiles : \n",
            "spark.submit.deployMode : client\n",
            "spark.ui.showConsoleProgress : true\n",
            "spark.executor.extraJavaOptions : -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **59. How to restrict the PySpark to use the number of cores in the system?**"
      ],
      "metadata": {
        "id": "2lOYRa0OVkj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf()\n",
        "conf.set(\"spark.executor.cores\", \"2\") # set the number of cores you want here\n",
        "sc = SparkContext(conf=conf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "kCgBPqliV-X2",
        "outputId": "6d849ffc-fa2e-4d5b-dc67-ce2fd78d3a93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=PySpark 101 Exercises, master=local[*]) created by getOrCreate at <ipython-input-2-83eafe77f2d8>:5 ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-32b385cc160e>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.executor.cores\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"2\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# set the number of cores you want here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    196\u001b[0m             )\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             self._do_init(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    446\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=PySpark 101 Exercises, master=local[*]) created by getOrCreate at <ipython-input-2-83eafe77f2d8>:5 "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **60. How to cache PySpark DataFrame or objects and delete cache?**"
      ],
      "metadata": {
        "id": "hZ2DWKtx04s_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Caching the DataFrame\n",
        "df.cache()\n",
        "\n",
        "# un-cache or unpersist data using the unpersist() method.\n",
        "df.unpersist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2QqO1Ob0xI0",
        "outputId": "1cac179a-261d-4042-db3b-6ffa73b82ba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Name: string, City: string]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **61. How to Divide a PySpark DataFrame randomly in a given ratio (0.8, 0.2)?**"
      ],
      "metadata": {
        "id": "PmLWcA2o1R0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)"
      ],
      "metadata": {
        "id": "VH2iGV5lHh-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **62. How to build logistic regression in PySpark?**"
      ],
      "metadata": {
        "id": "wi94FxNBIodg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sample dataframe\n",
        "data = spark.createDataFrame([\n",
        "(0, 1.0, -1.0),\n",
        "(1, 2.0, 1.0),\n",
        "(1, 3.0, -2.0),\n",
        "(0, 4.0, 1.0),\n",
        "(1, 5.0, -3.0),\n",
        "(0, 6.0, 2.0),\n",
        "(1, 7.0, -1.0),\n",
        "(0, 8.0, 3.0),\n",
        "(1, 9.0, -2.0),\n",
        "(0, 10.0, 2.0),\n",
        "(1, 11.0, -3.0),\n",
        "(0, 12.0, 1.0),\n",
        "(1, 13.0, -1.0),\n",
        "(0, 14.0, 2.0),\n",
        "(1, 15.0, -2.0),\n",
        "(0, 16.0, 3.0),\n",
        "(1, 17.0, -3.0),\n",
        "(0, 18.0, 1.0),\n",
        "(1, 19.0, -1.0),\n",
        "(0, 20.0, 2.0)\n",
        "], [\"label\", \"feat1\", \"feat2\"])"
      ],
      "metadata": {
        "id": "gyu-HaA1IlFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# convert the feature columns into a single vector column using VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=[\"feat1\", \"feat2\"], outputCol=\"features\")\n",
        "data = assembler.transform(data)\n",
        "\n",
        "# fit the logistic regression model\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "model = lr.fit(data)\n",
        "\n",
        "# look at the coefficients and intercept of the logistic regression model\n",
        "print(f\"Coefficients: {str(model.coefficients)}\")\n",
        "print(f\"Intercept: {str(model.intercept)}\")"
      ],
      "metadata": {
        "id": "WKNunKpNIyq1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65491060-7843-4057-d919-50d633079ffc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [0.02027774047578661,-1.612960940022365]\n",
            "Intercept: -0.22092927518295308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **63. How to convert the categorical string data into numerical data or index?**"
      ],
      "metadata": {
        "id": "RxyDJIwWIq8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sample DataFrame\n",
        "data = [('cat',), ('dog',), ('mouse',), ('fish',), ('dog',), ('cat',), ('mouse',)]\n",
        "df = spark.createDataFrame(data, [\"animal\"])\n",
        "df.show()"
      ],
      "metadata": {
        "id": "QOBFsbfYIvaY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bfe2684-162f-4294-d3f0-de12c4d03fcb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|animal|\n",
            "+------+\n",
            "|   cat|\n",
            "|   dog|\n",
            "| mouse|\n",
            "|  fish|\n",
            "|   dog|\n",
            "|   cat|\n",
            "| mouse|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "# Initialize a StringIndexer\n",
        "indexer = StringIndexer(inputCol='animal', outputCol='animalIndex')\n",
        "\n",
        "# Fit the indexer to the DataFrame and transform the data\n",
        "indexed = indexer.fit(df).transform(df)\n",
        "indexed.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGhbpBoLLXAX",
        "outputId": "36eb829b-fa0b-4ace-8a82-2edccf6ce789"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+\n",
            "|animal|animalIndex|\n",
            "+------+-----------+\n",
            "|   cat|        0.0|\n",
            "|   dog|        1.0|\n",
            "| mouse|        2.0|\n",
            "|  fish|        3.0|\n",
            "|   dog|        1.0|\n",
            "|   cat|        0.0|\n",
            "| mouse|        2.0|\n",
            "+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **64. How to calculate Correlation of two variables in a DataFrame?**"
      ],
      "metadata": {
        "id": "-9lqYIgpMqCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "# Create a sample dataframe\n",
        "data = [Row(feature1=5, feature2=10, feature3=25),\n",
        "Row(feature1=6, feature2=15, feature3=35),\n",
        "Row(feature1=7, feature2=25, feature3=30),\n",
        "Row(feature1=8, feature2=20, feature3=60),\n",
        "Row(feature1=9, feature2=30, feature3=70)]\n",
        "df = spark.createDataFrame(data)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uuoIH5ZMv9Q",
        "outputId": "58a6bf29-25cf-42e6-8543-55d0b57f5219"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+--------+\n",
            "|feature1|feature2|feature3|\n",
            "+--------+--------+--------+\n",
            "|       5|      10|      25|\n",
            "|       6|      15|      35|\n",
            "|       7|      25|      30|\n",
            "|       8|      20|      60|\n",
            "|       9|      30|      70|\n",
            "+--------+--------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.stat.corr('feature1', 'feature2'))\n",
        "print(df.stat.corr('feature1', 'feature3'))\n",
        "print(df.stat.corr('feature2', 'feature3'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7t89CKcvNUnn",
        "outputId": "be08c27e-7b4c-4f43-9664-4872dda119f5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9\n",
            "0.9177999171377654\n",
            "0.6783738517974789\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **65. How to calculate Correlation Matrix?**"
      ],
      "metadata": {
        "id": "8d4trUhPNsa8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sample dataframe\n",
        "data = [Row(feature1=5, feature2=10, feature3=25),\n",
        "Row(feature1=6, feature2=15, feature3=35),\n",
        "Row(feature1=7, feature2=25, feature3=30),\n",
        "Row(feature1=8, feature2=20, feature3=60),\n",
        "Row(feature1=9, feature2=30, feature3=70)]\n",
        "df = spark.createDataFrame(data)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8jbW5vQNux6",
        "outputId": "c679779d-becd-47e8-9453-2237f6d0e67e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+--------+\n",
            "|feature1|feature2|feature3|\n",
            "+--------+--------+--------+\n",
            "|       5|      10|      25|\n",
            "|       6|      15|      35|\n",
            "|       7|      25|      30|\n",
            "|       8|      20|      60|\n",
            "|       9|      30|      70|\n",
            "+--------+--------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.stat import Correlation\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# convert to vector column first\n",
        "vector_col = \"features\"\n",
        "assembler = VectorAssembler(inputCols=df.columns, outputCol=vector_col)\n",
        "df_vector = assembler.transform(df).select(vector_col)\n",
        "\n",
        "# get correlation matrix\n",
        "matrix = Correlation.corr(df_vector, vector_col).head()[0]\n",
        "\n",
        "print(matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecZafV3gOO7Y",
        "outputId": "9cc360c7-d288-42a6-e24a-3cc2163291f1"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DenseMatrix([[1.        , 0.9       , 0.91779992],\n",
            "             [0.9       , 1.        , 0.67837385],\n",
            "             [0.91779992, 0.67837385, 1.        ]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **66. How to calculate VIF (Variance Inflation Factor ) for set of variables in a DataFrame?**\n",
        "\n"
      ],
      "metadata": {
        "id": "xuzFDnr0OxJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sample dataframe\n",
        "data = [Row(feature1=5, feature2=10, feature3=25),\n",
        "Row(feature1=6, feature2=15, feature3=35),\n",
        "Row(feature1=7, feature2=25, feature3=30),\n",
        "Row(feature1=8, feature2=20, feature3=60),\n",
        "Row(feature1=9, feature2=30, feature3=70)]\n",
        "df = spark.createDataFrame(data)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5DIOmUmO2ap",
        "outputId": "b675b1d9-b3dd-42d4-cc09-c7654d5559b7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+--------+\n",
            "|feature1|feature2|feature3|\n",
            "+--------+--------+--------+\n",
            "|       5|      10|      25|\n",
            "|       6|      15|      35|\n",
            "|       7|      25|      30|\n",
            "|       8|      20|      60|\n",
            "|       9|      30|      70|\n",
            "+--------+--------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml.linalg import Vectors\n",
        "def calculate_vif(data, features):\n",
        "  vif_dict = {}\n",
        "\n",
        "  for feature in features:\n",
        "    non_feature_cols = [col for col in features if col != feature]\n",
        "    assembler = VectorAssembler(inputCols=non_feature_cols, outputCol=\"features\")\n",
        "    lr = LinearRegression(featuresCol='features', labelCol=feature)\n",
        "\n",
        "    model = lr.fit(assembler.transform(data))\n",
        "    vif = 1 / (1 - model.summary.r2)\n",
        "\n",
        "    vif_dict[feature] = vif\n",
        "  return vif_dict\n",
        "\n",
        "features = ['feature1', 'feature2', 'feature3']\n",
        "vif_values = calculate_vif(df, features)\n",
        "\n",
        "for feature, vif in vif_values.items():\n",
        "  print(f'VIF for {feature}: {vif}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkUzAL_mO4-v",
        "outputId": "47aba64c-0103-49c8-b7b2-19e63ab74f4d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VIF for feature1: 66.2109375000003\n",
            "VIF for feature2: 19.33593749999992\n",
            "VIF for feature3: 23.3046875000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **67. How to perform Chi-Square test?**"
      ],
      "metadata": {
        "id": "pg7JiRY_Q5jR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sample dataframe\n",
        "data = [(1, 0, 0, 1, 1),\n",
        "(2, 0, 1, 0, 0),\n",
        "(3, 1, 0, 0, 0),\n",
        "(4, 0, 0, 1, 1),\n",
        "(5, 0, 1, 1, 0)]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"id\", \"feature1\", \"feature2\", \"feature3\", \"label\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKVhDXP4Q_bw",
        "outputId": "d012d57f-45fe-47b8-d980-ab7509856d59"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+--------+--------+-----+\n",
            "| id|feature1|feature2|feature3|label|\n",
            "+---+--------+--------+--------+-----+\n",
            "|  1|       0|       0|       1|    1|\n",
            "|  2|       0|       1|       0|    0|\n",
            "|  3|       1|       0|       0|    0|\n",
            "|  4|       0|       0|       1|    1|\n",
            "|  5|       0|       1|       1|    0|\n",
            "+---+--------+--------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "assembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\", \"feature3\"], outputCol=\"features\")\n",
        "df = assembler.transform(df)\n",
        "\n",
        "from pyspark.ml.stat import ChiSquareTest\n",
        "\n",
        "r = ChiSquareTest.test(df, \"features\", \"label\").head()\n",
        "print(\"pValues: \" + str(r.pValues))\n",
        "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
        "print(\"statistics: \" + str(r.statistics))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7azPvrUfRft0",
        "outputId": "b8a41160-f847-46c2-be6c-b28d4afd68ce"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pValues: [0.36131042852617856,0.13603712811414348,0.1360371281141436]\n",
            "degreesOfFreedom: [1, 1, 1]\n",
            "statistics: [0.8333333333333335,2.2222222222222228,2.2222222222222223]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Chi-Square Test is a statistical hypothesis **test used to determine if there is a significant association between two categorical variables in a sample.** *It is based on the difference between the observed frequencies in each category and the frequencies that we would expect to see under the assumption of independence (i.e., no relationship between the variables).*\n",
        "\n",
        "The resulting test statistic follows a Chi-Square distribution when the null hypothesis of independence is true.\n",
        "\n",
        "https://www.machinelearningplus.com/pyspark/pyspark-chi-square-test/"
      ],
      "metadata": {
        "id": "k8ND530MRCZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **68. How to calculate the Standard Deviation?**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Pd7ZB7H2ThzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "data = [(\"James\", \"Sales\", 3000),\n",
        "(\"Michael\", \"Sales\", 4600),\n",
        "(\"Robert\", \"Sales\", 4100),\n",
        "(\"Maria\", \"Finance\", 3000),\n",
        "(\"James\", \"Sales\", 3000),\n",
        "(\"Scott\", \"Finance\", 3300),\n",
        "(\"Jen\", \"Finance\", 3900),\n",
        "(\"Jeff\", \"Marketing\", 3000),\n",
        "(\"Kumar\", \"Marketing\", 2000),\n",
        "(\"Saif\", \"Sales\", 4100)]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, [\"Employee\", \"Department\", \"Salary\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTILbkmXTj4r",
        "outputId": "0cfc067f-4a49-46ad-8b32-6c6af30a01af"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+------+\n",
            "|Employee|Department|Salary|\n",
            "+--------+----------+------+\n",
            "|   James|     Sales|  3000|\n",
            "| Michael|     Sales|  4600|\n",
            "|  Robert|     Sales|  4100|\n",
            "|   Maria|   Finance|  3000|\n",
            "|   James|     Sales|  3000|\n",
            "|   Scott|   Finance|  3300|\n",
            "|     Jen|   Finance|  3900|\n",
            "|    Jeff| Marketing|  3000|\n",
            "|   Kumar| Marketing|  2000|\n",
            "|    Saif|     Sales|  4100|\n",
            "+--------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "df.select(F.stddev(\"Salary\")).show()\n",
        "\n",
        "salary_stddev = df.select(F.stddev(\"Salary\").alias(\"stddev\"))\n",
        "salary_stddev.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HqtfvMeTwS7",
        "outputId": "42a09121-3e28-45be-ba23-da00d6c80d28"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "|stddev_samp(Salary)|\n",
            "+-------------------+\n",
            "|  765.9416862050705|\n",
            "+-------------------+\n",
            "\n",
            "+-----------------+\n",
            "|           stddev|\n",
            "+-----------------+\n",
            "|765.9416862050705|\n",
            "+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **69. How to calculate missing value percentage in each column?**"
      ],
      "metadata": {
        "id": "DowIaXhPUKmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sample dataframe\n",
        "data = [(\"John\", \"Doe\", None),\n",
        "(None, \"Smith\", \"New York\"),\n",
        "(\"Mike\", \"Smith\", None),\n",
        "(\"Anna\", \"Smith\", \"Boston\"),\n",
        "(None, None, None)]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"FirstName\", \"LastName\", \"City\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKGGcXt7UJ8E",
        "outputId": "c5002043-c68f-48c7-b97a-f02b04ea1202"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+--------+\n",
            "|FirstName|LastName|    City|\n",
            "+---------+--------+--------+\n",
            "|     John|     Doe|    null|\n",
            "|     null|   Smith|New York|\n",
            "|     Mike|   Smith|    null|\n",
            "|     Anna|   Smith|  Boston|\n",
            "|     null|    null|    null|\n",
            "+---------+--------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the total number of rows in the dataframe\n",
        "total_rows = df.count()\n",
        "\n",
        "# For each column calculate the number of null values and then calculate the percentage\n",
        "for column in df.columns:\n",
        "  null_values = df.filter(df[column].isNull()).count()\n",
        "  missing_percentage = (null_values / total_rows) * 100\n",
        "  print(f\"Missing values in {column}: {missing_percentage}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBFQRLnfWVah",
        "outputId": "2e6d4853-7e29-41ad-c8f8-98a661dc8842"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in FirstName: 40.0%\n",
            "Missing values in LastName: 20.0%\n",
            "Missing values in City: 60.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **70. How to get the names of DataFrame objects that have been created in an environment?**"
      ],
      "metadata": {
        "id": "c7xInrQfYVei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe_names = [name for name, obj in globals().items() if isinstance(obj, pyspark.sql.DataFrame)]\n",
        "\n",
        "for name in dataframe_names:\n",
        "  print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNXU3HrOYYV9",
        "outputId": "ca28d0f0-87b8-4aca-e16d-80431286b8e9"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pysparkDF\n",
            "train_data\n",
            "test_data\n",
            "df\n",
            "indexed\n",
            "df_vector\n",
            "salary_stddev\n",
            "amount_missing_df\n"
          ]
        }
      ]
    }
  ]
}